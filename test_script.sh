# python get_activations_and_probe_non_linear_supcon_bce.py hl_llama_7B strqa --train_file_name strqa_baseline_responses_train --test_file_name strqa_baseline_responses_test --len_dataset 1832 --num_folds 1 --using_act layer --token answer_last --method individual_linear_hallu_pos --bs 128 --epochs 50 --lr 0.00005 --norm_input True --save_probes True --save_path ~/Desktop/honest_llama_data --fast_mode True --plot_name linear-n-0.00005 --tag main
# python get_activations_and_probe_non_linear_supcon_bce.py hl_llama_7B strqa --train_file_name strqa_baseline_responses_train --test_file_name strqa_baseline_responses_test --len_dataset 1832 --num_folds 1 --using_act layer --token answer_last --method individual_linear_hallu_pos --bs 128 --epochs 50 --lr 0.0005 --norm_input True --save_probes True --save_path ~/Desktop/honest_llama_data --fast_mode True --plot_name linear-n-0.0005 --tag main
# python get_activations_and_probe_non_linear_supcon_bce.py hl_llama_7B strqa --train_file_name strqa_baseline_responses_train --test_file_name strqa_baseline_responses_test --len_dataset 1832 --num_folds 1 --using_act layer --token answer_last --method individual_linear_hallu_pos --bs 128 --epochs 50 --lr 0.005 --norm_input True --save_probes True --save_path ~/Desktop/honest_llama_data --fast_mode True --plot_name linear-n-0.005 --tag main
# python get_activations_and_probe_non_linear_supcon_bce.py hl_llama_7B strqa --train_file_name strqa_baseline_responses_train --test_file_name strqa_baseline_responses_test --len_dataset 1832 --num_folds 1 --using_act layer --token answer_last --method individual_linear_hallu_pos --bs 128 --epochs 50 --lr 0.05 --norm_input True --save_probes True --save_path ~/Desktop/honest_llama_data --fast_mode True --plot_name linear-n-0.05 --tag main
python get_activations_and_probe_non_linear_supcon_bce.py hl_llama_7B strqa --train_file_name strqa_sampledplus_responses_train --test_file_name strqa_baseline_responses_test --len_dataset 16479 --num_folds 1 --using_act layer --token answer_last --method individual_linear_hallu_pos --bs 128 --epochs 50 --lr 0.00005 --norm_input True --save_probes True --save_path ~/Desktop/honest_llama_data --fast_mode True --plot_name linear-n*aug-0.00005 --tag main --num_samples 9
python get_activations_and_probe_non_linear_supcon_bce.py hl_llama_7B strqa --train_file_name strqa_sampledplus_responses_train --test_file_name strqa_baseline_responses_test --len_dataset 16479 --num_folds 1 --using_act layer --token answer_last --method individual_linear_hallu_pos --bs 128 --epochs 50 --lr 0.0005 --norm_input True --save_probes True --save_path ~/Desktop/honest_llama_data --fast_mode True --plot_name linear-n*aug-0.0005 --tag main --num_samples 9
python get_activations_and_probe_non_linear_supcon_bce.py hl_llama_7B strqa --train_file_name strqa_sampledplus_responses_train --test_file_name strqa_baseline_responses_test --len_dataset 16479 --num_folds 1 --using_act layer --token answer_last --method individual_linear_hallu_pos --bs 128 --epochs 50 --lr 0.005 --norm_input True --save_probes True --save_path ~/Desktop/honest_llama_data --fast_mode True --plot_name linear-n*aug-0.005 --tag main --num_samples 9
python get_activations_and_probe_non_linear_supcon_bce.py hl_llama_7B strqa --train_file_name strqa_sampledplus_responses_train --test_file_name strqa_baseline_responses_test --len_dataset 16479 --num_folds 1 --using_act layer --token answer_last --method individual_linear_hallu_pos --bs 128 --epochs 50 --lr 0.05 --norm_input True --save_probes True --save_path ~/Desktop/honest_llama_data --fast_mode True --plot_name linear-n*aug-0.05 --tag main --num_samples 9
# python get_activations_and_probe_transformer.py hl_llama_7B strqa --train_file_name strqa_baseline_responses_train --test_file_name strqa_baseline_responses_test --len_dataset 1832 --num_folds 1 --using_act layer --token answer_last --method transformer2_hallu_pos --bs 128 --epochs 50 --lr 0.00005 --norm_input True --save_probes True --save_path ~/Desktop/honest_llama_data --fast_mode True --plot_name transformer-n-0.00005 --tag main
# python get_activations_and_probe_transformer.py hl_llama_7B strqa --train_file_name strqa_baseline_responses_train --test_file_name strqa_baseline_responses_test --len_dataset 1832 --num_folds 1 --using_act layer --token answer_last --method transformer2_hallu_pos --bs 128 --epochs 50 --lr 0.0005 --norm_input True --save_probes True --save_path ~/Desktop/honest_llama_data --fast_mode True --plot_name transformer-n-0.0005 --tag main
# python get_activations_and_probe_transformer.py hl_llama_7B strqa --train_file_name strqa_baseline_responses_train --test_file_name strqa_baseline_responses_test --len_dataset 1832 --num_folds 1 --using_act layer --token answer_last --method transformer2_hallu_pos --bs 128 --epochs 50 --lr 0.005 --norm_input True --save_probes True --save_path ~/Desktop/honest_llama_data --fast_mode True --plot_name transformer-n-0.005 --tag main
# python get_activations_and_probe_transformer.py hl_llama_7B strqa --train_file_name strqa_baseline_responses_train --test_file_name strqa_baseline_responses_test --len_dataset 1832 --num_folds 1 --using_act layer --token answer_last --method transformer2_hallu_pos --bs 128 --epochs 50 --lr 0.05 --norm_input True --save_probes True --save_path ~/Desktop/honest_llama_data --fast_mode True --plot_name transformer-n-0.05 --tag main
python get_activations_and_probe_transformer.py hl_llama_7B strqa --train_file_name strqa_sampledplus_responses_train --test_file_name strqa_baseline_responses_test --len_dataset 16479 --num_folds 1 --using_act layer --token answer_last --method transformer2_hallu_pos --bs 128 --epochs 50 --lr 0.00005 --norm_input True --save_probes True --save_path ~/Desktop/honest_llama_data --fast_mode True --plot_name transformer-n*aug-0.00005 --tag main --num_samples 9
python get_activations_and_probe_transformer.py hl_llama_7B strqa --train_file_name strqa_sampledplus_responses_train --test_file_name strqa_baseline_responses_test --len_dataset 16479 --num_folds 1 --using_act layer --token answer_last --method transformer2_hallu_pos --bs 128 --epochs 50 --lr 0.0005 --norm_input True --save_probes True --save_path ~/Desktop/honest_llama_data --fast_mode True --plot_name transformer-n*aug-0.0005 --tag main --num_samples 9
python get_activations_and_probe_transformer.py hl_llama_7B strqa --train_file_name strqa_sampledplus_responses_train --test_file_name strqa_baseline_responses_test --len_dataset 16479 --num_folds 1 --using_act layer --token answer_last --method transformer2_hallu_pos --bs 128 --epochs 50 --lr 0.005 --norm_input True --save_probes True --save_path ~/Desktop/honest_llama_data --fast_mode True --plot_name transformer-n*aug-0.005 --tag main --num_samples 9
python get_activations_and_probe_transformer.py hl_llama_7B strqa --train_file_name strqa_sampledplus_responses_train --test_file_name strqa_baseline_responses_test --len_dataset 16479 --num_folds 1 --using_act layer --token answer_last --method transformer2_hallu_pos --bs 128 --epochs 50 --lr 0.05 --norm_input True --save_probes True --save_path ~/Desktop/honest_llama_data --fast_mode True --plot_name transformer-n*aug-0.05 --tag main --num_samples 9