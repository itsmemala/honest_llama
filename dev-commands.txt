python get_baselines.py alpaca_7B nq_open --train_labels_file_name nq_open_greedy_responses_labels_train5000 --test_labels_file_name nq_open_greedy_responses_labels_1800 --train_uncertainty_values_file_name greedy_responses_train5000 --test_uncertainty_values_file_name greedy_responses_1800 --len_dataset 1500 --num_folds 1 --save_path ~/Desktop/honest_llama_data

python analyse_results.py --results_file_name llama_7B_trivia_qa_greedy_responses_train5000_5000_1_layer_answer_last_individual_linear_bs128_epochs20_0.05_Adam_w_lr_sch_False_None_None --save_path ~/Desktop/honest_llama_data --use_similarity True

python analyse_results.py --results_file_name alpaca_7B_trivia_qa_greedy_responses_train5000_5000_1_ah_answer_last_individual_linear_bs128_epochs20_0.05_Adam_w_lr_sch_False_None_None --save_path ~/Desktop/honest_llama_data --layer_start 0,4,7,10,13,16,19,22,25,28 --layer_end 3,6,9,12,15,18,21,24,27,31

python get_activations_and_probe.py alpaca_7B trivia_qa --train_file_name trivia_qa_greedy_responses_train5000 --train_labels_file_name trivia_qa_greedy_responses_labels_train5000 --test_file_name trivia_qa_greedy_responses_validation1800 --test_labels_file_name trivia_qa_greedy_responses_labels_validation1800 --len_dataset 5000 --num_folds 1 --using_act mlp --token answer_last --method individual_linear --use_unitnorm True --bs 128 --epochs 20 --lr 0.05 --optimizer Adam_w_lr_sch --save_probes True --device 0 --save_path ~/Desktop/honest_llama_data

python get_activations_and_probe.py llama_7B nq_open --train_file_name nq_open_greedy_responses_train5000 --train_labels_file_name nq_open_greedy_responses_labels_train5000 --test_file_name nq_open_greedy_responses_validation1800 --test_labels_file_name nq_open_greedy_responses_labels_validation1800 --len_dataset 5000 --num_folds 1 --using_act mlp --token answer_last --method individual_linear --use_unitnorm False --bs 128 --epochs 20 --lr 0.05 --optimizer Adam_w_lr_sch --classifier_on_probes True --save_probes True --device 0 --save_path ~/Desktop/honest_llama_data

python get_activations_and_probe_non_linear_supcon_bce.py llama_7B trivia_qa --train_file_name trivia_qa_greedy_responses_train5000 --train_labels_file_name trivia_qa_greedy_responses_labels_train5000 --test_file_name trivia_qa_greedy_responses_validation1800 --test_labels_file_name trivia_qa_greedy_responses_labels_validation1800 --len_dataset 5000 --num_folds 1 --using_act layer --token answer_last --method individual_linear_orthogonal_hallu_pos --bs 128 --epochs 20 --lr 0.005 --save_probes True --device 0 --save_path ~/Desktop/honest_llama_data --spl_wgt 100 --excl_ce True

python get_activations_and_probe.py llama_7B trivia_qa --train_file_name trivia_qa_greedy_responses_train5000 --train_labels_file_name trivia_qa_greedy_responses_labels_train5000 --test_file_name trivia_qa_greedy_responses_validation1800 --test_labels_file_name trivia_qa_greedy_responses_labels_validation1800 --len_dataset 5000 --num_folds 1 --using_act mlp --token answer_last --method individual_linear_kld --use_unitnorm True --kld_wgt 1 --bs 128 --epochs 20 --lr 0.05 --optimizer Adam_w_lr_sch --save_probes True --device 0 --save_path ~/Desktop/honest_llama_data --custom_layers 
14,24,20,19,29

18 23 19 12 31

13,12,19,23,1

python get_activations_and_probe.py hl_llama_7B gsm8k --train_file_name gsm8k_baseline_responses --len_dataset 1319 --num_folds 1 --using_act layer --token answer_last --method individual_linear --use_linear_bias True --bs 128 --epochs 20 --lr 0.05 --optimizer Adam_w_lr_sch --save_probes True --device 0 --save_path ~/Desktop/honest_llama_data

python get_activations_and_probe.py hl_llama_7B strqa --train_file_name strqa_baseline_responses_train --test_file_name strqa_baseline_responses_test --len_dataset 1832 --num_folds 1 --using_act layer --token answer_last --method individual_linear --use_linear_bias True --bs 128 --epochs 20 --lr 0.05 --optimizer Adam_w_lr_sch --save_probes True --device 0 --save_path ~/Desktop/honest_llama_data

python get_activations_and_probe_non_linear_supcon_bce.py hl_llama_7B trivia_qa --train_file_name trivia_qa_baseline_responses_train --test_file_name trivia_qa_baseline_responses_test --len_dataset 5000 --num_folds 1 --using_act layer --token answer_last --method individual_non_linear_2_hallu_pos --bs 128 --epochs 20 --lr 0.005 --save_probes True --device 0 --save_path ~/Desktop/honest_llama_data --fast_mode True

python get_activations_and_probe_transformer.py alpaca_7B trivia_qa --train_file_name trivia_qa_greedy_responses_train5000 --train_labels_file_name trivia_qa_greedy_responses_labels_train5000 --test_file_name trivia_qa_greedy_responses_validation1800 --test_labels_file_name trivia_qa_greedy_responses_labels_validation1800 --len_dataset 5000 --num_folds 1 --using_act layer --token tagged_tokens --method transformer_hallu_pos --bs 128 --epochs 100 --lr 0.00001 --save_probes True --device 0 --save_path ~/Desktop/honest_llama_data --fast_mode True

python analyse_crossdataset_bce.py hl_llama_7B trivia_qa --responses_file_name baseline_responses_test --probes_file_name NLSC_hl_llama_7B_trivia_qa_baseline_responses_train_5000_1_layer_answer_last_individual_non_linear_2_hallu_pos_bs128_epochs20_0.005_False --save_path ~/Desktop/honest_llama_data --mitigated_responses_file_name dola16to32_responses_test

------------------------------------------
4) Another approach is to go selfsupervised, try to ask the model to correct its answers and say that the answer is hallucinated. Gather the responses:
Given a question, extract different responses from the normal questions (maybe 2 suffices), and responses after commenting that the answer is hallucinated. Treat the two normal answers as positives and the answer on the hallucination as negative. Train with contrastive loss. Maybe then train a classifier or estimates prototypes.. to use for classifying test samples based on one answer.
-------------------------------------------

##################### Active commands
python analyse_crossdataset_bce.py alpaca_7B trivia_qa --probes_file_name NLSC_alpaca_7B_trivia_qa_greedy_responses_train20000_20000_1_layer_answer_last_individual_non_linear_2_supcon_hallu_pos_256_250_0.005_0.1_bs128_epochs20_0.05_False --save_path ~/Desktop/honest_llama_data --best_threshold True

python get_prompt_responses_mi.py llama_7B --device 0 --save_path ~/Desktop/honest_llama_data --prompt_type E

/home/malavika/anaconda3/envs/iti/lib/python3.8/site-packages/transformers/generation/

python get_activations_and_probe_transformer.py alpaca_7B nq_open --train_file_name nq_open_greedy_responses_train5000 --train_labels_file_name nq_open_greedy_responses_labels_train5000 --test_file_name nq_open_greedy_responses_validation1800 --test_labels_file_name nq_open_greedy_responses_labels_validation1800 --len_dataset 5000 --num_folds 1 --using_act layer --token least_likely --method transformer_hallu_pos --bs 128 --epochs 100 --lr 0.00001 --save_probes True --device 0 --save_path ~/Desktop/honest_llama_data --fast_mode True

python get_activations_and_probe_transformer.py hl_llama_7B strqa --train_file_name strqa_baseline_responses_train --test_file_name strqa_baseline_responses_test --len_dataset 1832 --num_folds 1 --using_act layer --token prompt_last --method transformer_hallu_pos --bs 128 --epochs 100 --lr 0.00001 --save_probes True --device 0 --save_path ~/Desktop/honest_llama_data --fast_mode True

## llama 2 ################################################################
python get_activations_and_probe_non_linear_supcon_bce.py llama_2_7B trivia_qa --train_file_name trivia_qa_greedy_responses_train2000 --test_file_name trivia_qa_greedy_responses_validation2000 --train_labels_file_name trivia_qa_sampled_responses_train2000_se_labels --test_labels_file_name trivia_qa_sampled_responses_validation2000_se_labels --len_dataset 2000 --num_folds 1 --using_act layer --token slt --method individual_linear_hallu_pos --bs 128 --epochs 20 --lr 0.005 --save_probes True --device 0 --save_path ~/Desktop/honest_llama_data --fast_mode True

python get_activations_and_probe_non_linear_supcon_bce.py llama_2_7B trivia_qa --train_file_name trivia_qa_greedy_responses_train2000 --test_file_name trivia_qa_greedy_responses_validation2000 --train_labels_file_name trivia_qa_greedy_responses_labels_train2000 --test_labels_file_name trivia_qa_greedy_responses_labels_validation2000 --len_dataset 2000 --num_folds 1 --using_act layer --token answer_last --method individual_non_linear_2_supcon_hallu_pos --bs 165 --epochs 250 --lr 0.0005 --save_probes True --device 0 --save_path ~/Desktop/honest_llama_data --fast_mode True

python analyse_crossdataset_bce.py llama_2_7B trivia_qa --probes_file_name NLSC_llama_2_7B_trivia_qa_greedy_responses_train2000_2000_1_layer_answer_last_individual_non_linear_2_supcon_hallu_pos_128_10_0.05_0.1_bs165_epochs250_0.0005_False --save_path ~/honest_llama_data --best_threshold True

python plot_curves.py llama_2_7B trivia_qa --save_path ~/Desktop/honest_llama_data --probes_file_name NLSC_llama_2_7B_trivia_qa_greedy_responses_train2000_2000_1_layer_answer_last_individual_non_linear_2_supcon_hallu_pos_128_10_0.05_0.1_bs165_epochs250_0.0005_False --using_act layer --token answer_last --method last_layer_supcon --bs 165 --lr 0.0005

python get_activations_and_probe_non_linear_supcon_bce.py llama_2_7B trivia_qa --train_file_name trivia_qa_sampledplus_responses_train2000 --test_file_name trivia_qa_greedy_responses_validation2000 --train_labels_file_name trivia_qa_sampledplus_responses_labels_train2000 --test_labels_file_name trivia_qa_greedy_responses_labels_validation2000 --len_dataset 22000 --num_folds 1 --using_act layer --token answer_last --method individual_non_linear_2_supcon_hallu_pos --bs 165 --epochs 250 --lr 0.005 --save_probes True --save_path ~/honest_llama_data --fast_mode True --num_samples 11 --no_batch_sampling True --multi_gpu True

python analyse_crossdataset_bce.py llama_2_7B trivia_qa --probes_file_name NLSC_llama_2_7B_trivia_qa_sampledplus_responses_train2000_22000_1_layer_answer_last_individual_non_linear_2_supcon_hallu_pos_128_10_0.05_0.1_bs165_epochs250_0.0005_False --save_path ~/Desktop/honest_llama_data --best_threshold True

python plot_curves.py llama_2_7B trivia_qa --save_path ~/Desktop/honest_llama_data --probes_file_name NLSC_llama_2_7B_trivia_qa_sampledplus_responses_train2000_22000_1_layer_answer_last_individual_non_linear_2_supcon_hallu_pos_128_10_0.05_0.1_bs165_epochs250_0.0005_False --using_act layer --token answer_last --method last_layer_supcon --bs 165 --lr 0.0005

python get_activations_and_probe_transformer.py llama_2_7B trivia_qa --train_file_name trivia_qa_greedy_responses_train2000 --train_labels_file_name trivia_qa_greedy_responses_labels_train2000 --test_file_name trivia_qa_greedy_responses_validation2000 --test_labels_file_name trivia_qa_greedy_responses_labels_validation2000 --len_dataset 2000 --num_folds 1 --using_act layer --token answer_last --method transformer_supcon_hallu_pos --bs 165 --epochs 250 --lr 0.0001 --save_probes True --save_path ~/honest_llama_data --fast_mode True --num_samples 11 --multi_gpu True

python get_activations_and_probe_transformer.py llama_2_7B trivia_qa --train_file_name trivia_qa_sampledplus_responses_train2000 --train_labels_file_name trivia_qa_sampledplus_responses_labels_train2000 --test_file_name trivia_qa_greedy_responses_validation2000 --test_labels_file_name trivia_qa_greedy_responses_labels_validation2000 --len_dataset 22000 --num_folds 1 --using_act layer --token answer_last --method transformer_supcon_hallu_pos --bs 165 --epochs 250 --lr 0.0001 --save_probes True --save_path ~/honest_llama_data --fast_mode True --num_samples 11 --no_batch_sampling True --multi_gpu True

python plot_curves.py llama_2_7B trivia_qa --save_path ~/Desktop/honest_llama_data --probes_file_name T_llama_2_7B_trivia_qa_greedy_responses_train2000_2000_1_layer_answer_last_transformer_hallu_pos_bs128_epochs100_1e-05_False --using_act layer --token answer_last --method transformer --bs 128 --lr 0.00001 --num_blocks 1 --wd 0.00001 --with_pe True --norm_inp True
--norm_inp True --with_pe True


## strqa sampledplus ########################################################
python get_activations_and_probe_non_linear_supcon_bce.py hl_llama_7B strqa --train_file_name strqa_sampledplus_responses_train --test_file_name strqa_baseline_responses_test --len_dataset 16479 --num_folds 1 --using_act layer --token answer_last --method individual_non_linear_2_supcon_hallu_pos --bs 160 --epochs 250 --lr 0.0005 --save_probes True --save_path ~/honest_llama_data --fast_mode True --num_samples 9 --no_batch_sampling True --multi_gpu True

python get_activations_and_probe_transformer.py hl_llama_7B strqa --train_file_name strqa_sampledplus_responses_train --test_file_name strqa_baseline_responses_test --len_dataset 16479 --num_folds 1 --using_act layer --token answer_last --method transformer_supcon_hallu_pos --bs 160 --epochs 250 --lr 0.0001 --save_probes True --save_path ~/honest_llama_data --fast_mode True --num_samples 9 --no_batch_sampling True --multi_gpu True

python analyse_crossdataset_bce.py hl_llama_7B strqa --probes_file_name NLSC_hl_llama_7B_strqa_sampledplus_responses_train_16479_1_layer_answer_last_individual_non_linear_2_supcon_hallu_pos_128_10_0.05_0.1_bs160_epochs250_0.0005_False --save_path ~/Desktop/honest_llama_data --best_threshold True


## alpaca - trivia qa ######################################
python get_activations_and_probe_non_linear_supcon_bce.py alpaca_7B trivia_qa --train_file_name trivia_qa_sampledplus_responses_train2000 --test_file_name trivia_qa_greedy_responses_validation1800 --train_labels_file_name trivia_qa_sampledplus_responses_labels_train2000 --test_labels_file_name trivia_qa_greedy_responses_labels_validation1800 --len_dataset 22000 --num_folds 1 --using_act layer --token answer_last --method individual_linear_hallu_pos --bs 128 --epochs 20 --lr 0.005 --save_probes True --save_path ~/Desktop/honest_llama_data --fast_mode True --num_samples 11

python analyse_crossdataset_bce.py alpaca_7B trivia_qa --probes_file_name NLSC_alpaca_7B_trivia_qa_sampledplus_responses_train2000_22000_1_layer_answer_last_individual_non_linear_2_supcon_hallu_pos_128_10_0.05_0.1_bs165_epochs250_0.005_False --save_path ~/Desktop/honest_llama_data --best_threshold True

python plot_curves.py alpaca_7B trivia_qa --save_path ~/Desktop/honest_llama_data --probes_file_name NLSC_alpaca_7B_trivia_qa_sampledplus_responses_train2000_22000_1_layer_answer_last_individual_non_linear_2_supcon_hallu_pos_128_10_0.05_0.1_bs165_epochs250_0.005_False --using_act layer --token answer_last --method last_layer_supcon --bs 165 --lr 0.005

python get_activations_and_probe_transformer.py alpaca_7B trivia_qa --train_file_name trivia_qa_sampledplus_responses_train2000 --train_labels_file_name trivia_qa_sampledplus_responses_labels_train2000 --test_file_name trivia_qa_greedy_responses_validation1800 --test_labels_file_name trivia_qa_greedy_responses_labels_validation1800 --len_dataset 22000 --num_folds 1 --using_act layer --token answer_last --method transformer_supcon_hallu_pos --bs 165 --epochs 1000 --lr 0.00001 --save_probes True --save_path ~/honest_llama_data --fast_mode True --num_samples 11 --no_batch_sampling True

python plot_curves.py alpaca_7B trivia_qa --save_path ~/Desktop/honest_llama_data --probes_file_name T_alpaca_7B_trivia_qa_sampledplus_responses_train2000_22000_1_layer_answer_last_transformer_supcon_hallu_pos_bs165_epochs250_0.0001_False --using_act layer --token answer_last --method transformer_supcon --bs 165 --lr 0.0001






