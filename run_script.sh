# python get_activations.py alpaca_7B nq_open --token tagged_tokens --file_name nq_open_greedy_responses_train5000  --device 0 --save_path ~/Desktop/honest_llama_data
# python get_activations.py alpaca_7B nq_open --token tagged_tokens --file_name nq_open_greedy_responses_validation1800  --device 0 --save_path ~/Desktop/honest_llama_data
# python get_activations_and_probe_transformer.py alpaca_7B nq_open --train_file_name nq_open_greedy_responses_train5000 --train_labels_file_name nq_open_greedy_responses_labels_train5000 --test_file_name nq_open_greedy_responses_validation1800 --test_labels_file_name nq_open_greedy_responses_labels_validation1800 --len_dataset 5000 --num_folds 1 --using_act layer --token tagged_tokens --method transformer_hallu_pos --bs 32 --epochs 50 --lr 0.0001 --norm_input True --save_probes True --device 0 --save_path ~/Desktop/honest_llama_data --max_tokens 40 --use_pe True


## HL-Llama-7B TriviaQA ##
## Ablations - baseline probes##

## Ablations - transformer probes##

##########################################################################

## HL-Llama-7B NQ ##
## Ablations - baseline probes##

## Ablations - transformer probes##

## Additional - tokens ##

## Main ##
python get_activations_and_probe_non_linear_supcon_bce.py hl_llama_7B nq_open --train_file_name nq_open_greedy_responses_train2000 --test_file_name nq_open_greedy_responses_validation1800 --train_labels_file_name nq_open_greedy_responses_labels_train2000 --test_labels_file_name nq_open_greedy_responses_labels_validation1800 --len_dataset 2000 --num_folds 1 --using_act layer --token answer_last --method individual_linear_hallu_pos --bs 128 --epochs 50 --lr 0.5 --norm_input True --save_probes True --save_path /content/gdrive/MyDrive/honest_llama --fast_mode True  --plot_name linear-n-0.5 --tag main
python get_activations_and_probe_non_linear_supcon_bce.py hl_llama_7B nq_open --train_file_name nq_open_sampledplus_responses_train2000 --test_file_name nq_open_greedy_responses_validation1800 --train_labels_file_name nq_open_sampledplus_responses_labels_train2000 --test_labels_file_name nq_open_greedy_responses_labels_validation1800 --len_dataset 22000 --num_folds 1 --using_act layer --token answer_last --method individual_linear_hallu_pos --bs 128 --epochs 50 --lr 0.005 --norm_input True --save_probes True --save_path /content/gdrive/MyDrive/honest_llama --fast_mode True  --plot_name linear-n*aug-0.005 --tag main --num_samples 11
python get_activations_and_probe_transformer.py hl_llama_7B nq_open --train_file_name nq_open_greedy_responses_train2000 --train_labels_file_name nq_open_greedy_responses_labels_train2000 --test_file_name nq_open_greedy_responses_validation1800 --test_labels_file_name nq_open_greedy_responses_labels_validation1800 --len_dataset 2000 --num_folds 1 --using_act layer --token answer_last --method transformer2_hallu_pos --bs 128 --epochs 50 --lr 0.00005 --norm_input True --save_probes True --save_path /content/gdrive/MyDrive/honest_llama --fast_mode True --plot_name transformer-n-0.00005 --tag main
python get_activations_and_probe_transformer.py hl_llama_7B nq_open --train_file_name nq_open_sampledplus_responses_train2000 --train_labels_file_name nq_open_sampledplus_responses_labels_train2000 --test_file_name nq_open_greedy_responses_validation1800 --test_labels_file_name nq_open_greedy_responses_labels_validation1800 --len_dataset 22000 --num_folds 1 --using_act layer --token answer_last --method transformer2_hallu_pos --bs 128 --epochs 50 --lr 0.0005 --norm_input True --save_probes True --save_path /content/gdrive/MyDrive/honest_llama --fast_mode True --plot_name transformer-n*aug-0.0005 --tag main --num_samples 11

##########################################################################

## Hl-Llama-7B StrQA ##
## Ablations - baseline probes##
python get_activations_and_probe_non_linear_supcon_bce.py hl_llama_7B strqa --train_file_name strqa_baseline_responses_train --test_file_name strqa_baseline_responses_test --len_dataset 1832 --num_folds 1 --using_act layer --token answer_last --method individual_linear_hallu_pos --bs 128 --epochs 50 --lr 0.005 --save_probes True --save_path ~/Desktop/honest_llama_data --fast_mode True --plot_name linear-n-raw-0.005 --tag ablation
python get_activations_and_probe_non_linear_supcon_bce.py hl_llama_7B strqa --train_file_name strqa_baseline_responses_train --test_file_name strqa_baseline_responses_test --len_dataset 1832 --num_folds 1 --using_act layer --token answer_last --method individual_linear_hallu_pos --bs 128 --epochs 50 --lr 0.05 --norm_input True --save_probes True --save_path ~/Desktop/honest_llama_data --fast_mode True --plot_name linear-n-norm-0.05 --tag ablation
python get_activations_and_probe_non_linear_supcon_bce.py hl_llama_7B strqa --train_file_name strqa_baseline_responses_train --test_file_name strqa_baseline_responses_test --len_dataset 1832 --num_folds 1 --using_act layer --token answer_last --method individual_linear_hallu_pos --bs 128 --epochs 50 --lr 0.005 --norm_input True --save_probes True --save_path ~/Desktop/honest_llama_data --fast_mode True --plot_name linear-n-meannorm-0.005 --tag ablation
python get_activations_and_probe_non_linear_supcon_bce.py hl_llama_7B strqa --train_file_name strqa_baseline_responses_train --test_file_name strqa_baseline_responses_test --len_dataset 1832 --num_folds 1 --using_act layer --token answer_last --method individual_non_linear_4_hallu_pos --bs 128 --epochs 50 --lr 0.00005 --save_probes True --save_path ~/Desktop/honest_llama_data --fast_mode True --plot_name nlinear-n-0.00005b --tag ablation --use_best_val_t True
python get_activations_and_probe_non_linear_supcon_bce.py hl_llama_7B strqa --train_file_name strqa_baseline_responses_train --test_file_name strqa_baseline_responses_test --len_dataset 1832 --num_folds 1 --using_act layer --token answer_last --method individual_non_linear_4_hallu_pos --bs 128 --epochs 50 --lr 0.0005 --norm_input True --save_probes True --save_path ~/Desktop/honest_llama_data --fast_mode True --plot_name nlinear-n-norm-0.0005b --tag ablation --use_best_val_t True

## Ablations - transformer probes##
python get_activations_and_probe_transformer.py hl_llama_7B strqa --train_file_name strqa_baseline_responses_train --test_file_name strqa_baseline_responses_test --len_dataset 1832 --num_folds 1 --using_act layer --token answer_last --method transformer_hallu_pos --bs 128 --epochs 50 --lr 0.0005 --save_probes True --save_path ~/Desktop/honest_llama_data --fast_mode True --plot_name transformer-n-raw-0.0005 --tag ablation
python get_activations_and_probe_transformer.py hl_llama_7B strqa --train_file_name strqa_baseline_responses_train --test_file_name strqa_baseline_responses_test --len_dataset 1832 --num_folds 1 --using_act layer --token answer_last --method transformer_hallu_pos --bs 128 --epochs 50 --lr 0.00005 --norm_input True --save_probes True --save_path ~/Desktop/honest_llama_data --fast_mode True --plot_name transformer-n-norm-0.00005 --tag ablation
python get_activations_and_probe_transformer.py hl_llama_7B strqa --train_file_name strqa_baseline_responses_train --test_file_name strqa_baseline_responses_test --len_dataset 1832 --num_folds 1 --using_act layer --token answer_last --method transformer_hallu_pos --bs 128 --epochs 50 --lr 0.00005 --norm_input True --save_probes True --save_path ~/Desktop/honest_llama_data --fast_mode True --plot_name transformer-n-meannorm-0.00005 --tag ablation
python get_activations_and_probe_transformer.py hl_llama_7B strqa --train_file_name strqa_baseline_responses_train --test_file_name strqa_baseline_responses_test --len_dataset 1832 --num_folds 1 --using_act layer --token answer_last --method transformer_hallu_pos --bs 128 --epochs 50 --lr 0.00005 --save_probes True --save_path ~/Desktop/honest_llama_data --fast_mode True --plot_name transformer-n-raw-rawemb-0.00005 --tag ablation
python get_activations_and_probe_transformer.py hl_llama_7B strqa --train_file_name strqa_baseline_responses_train --test_file_name strqa_baseline_responses_test --len_dataset 1832 --num_folds 1 --using_act layer --token answer_last --method transformer_hallu_pos --bs 128 --epochs 50 --lr 0.00005 --norm_input True --save_probes True --save_path ~/Desktop/honest_llama_data --fast_mode True --plot_name transformer-n-norm-rawemb-0.00005 --tag ablation
python get_activations_and_probe_transformer.py hl_llama_7B strqa --train_file_name strqa_baseline_responses_train --test_file_name strqa_baseline_responses_test --len_dataset 1832 --num_folds 1 --using_act layer --token answer_last --method transformer_hallu_pos --bs 128 --epochs 50 --lr 0.00005 --norm_input True --save_probes True --save_path ~/Desktop/honest_llama_data --fast_mode True --plot_name transformer-n-meannorm-rawemb-0.00005 --tag ablation
python get_activations_and_probe_transformer.py hl_llama_7B strqa --train_file_name strqa_baseline_responses_train --test_file_name strqa_baseline_responses_test --len_dataset 1832 --num_folds 1 --using_act layer --token answer_last --method transformer_hallu_pos --bs 128 --epochs 50 --lr 0.00005 --norm_input True --save_probes True --save_path ~/Desktop/honest_llama_data --fast_mode True --plot_name transformer-n-norm-rawemb-pe-0.00005 --tag ablation
python get_activations_and_probe_transformer.py hl_llama_7B strqa --train_file_name strqa_baseline_responses_train --test_file_name strqa_baseline_responses_test --len_dataset 1832 --num_folds 1 --using_act layer --token answer_last --method transformer2_hallu_pos --bs 128 --epochs 50 --lr 0.00005 --norm_input True --save_probes True --save_path ~/Desktop/honest_llama_data --fast_mode True --plot_name transformer2-n-norm-rawemb-0.00005 --tag ablation

## Additional - tokens ##
python get_activations_and_probe_transformer.py hl_llama_7B strqa --train_file_name strqa_baseline_responses_train --test_file_name strqa_baseline_responses_test --len_dataset 1832 --num_folds 1 --using_act layer --token tagged_tokens --method transformer2_hallu_pos --bs 32 --epochs 50 --lr 0.00005 --norm_input True --save_probes True --device 0 --save_path ~/Desktop/honest_llama_data --max_tokens 40 --use_pe True --plot_name transformer-n-tagged-0.00005 --tag additional_tokens

## Additional - OOD ##
python get_activations_and_probe_transformer.py hl_llama_7B strqa --train_file_name strqa_baseline_responses_train --test_file_name nq_open_greedy_responses_validation1800 --test_labels_file_name nq_open_greedy_responses_labels_validation1800 --len_dataset 1832 --num_folds 1 --using_act layer --token answer_last --method transformer2_hallu_pos --bs 128 --epochs 50 --lr 0.00005 --norm_input True --save_probes True --save_path ~/Desktop/honest_llama_data --fast_mode True --plot_name nq-transformer-n --tag ood
python get_activations_and_probe_transformer.py hl_llama_7B strqa --train_file_name strqa_sampledplus_responses_train --test_file_name nq_open_greedy_responses_validation1800 --test_labels_file_name nq_open_greedy_responses_labels_validation1800 --len_dataset 16479 --num_folds 1 --using_act layer --token answer_last --method transformer2_hallu_pos --bs 128 --epochs 50 --lr 0.00005 --norm_input True --save_probes True --save_path ~/Desktop/honest_llama_data --fast_mode True --plot_name nq-transformer-n*aug --tag ood --num_samples 9
python get_activations_and_probe_transformer.py hl_llama_7B strqa --train_file_name strqa_sampledplus_responses_train --test_file_name nq_open_greedy_responses_validation1800 --test_labels_file_name nq_open_greedy_responses_labels_validation1800 --len_dataset 16479 --num_folds 1 --using_act layer --token answer_last --method transformer2_hallu_pos --bs 128 --epochs 50 --lr 0.00005 --norm_input True --save_probes True --save_path ~/Desktop/honest_llama_data --fast_mode True --plot_name nq-transformer-cur-n*aug --tag ood --num_samples 9 --retrain_model_path T_hl_llama_7B_strqa_baseline_responses_train_1832_1_layer_answer_last_transformer2_hallu_pos_bs128_epochs50_5e-05_False
python get_activations_and_probe_transformer.py hl_llama_7B strqa --train_file_name strqa_sampledplus_responses_train --test_file_name nq_open_greedy_responses_validation1800 --test_labels_file_name nq_open_greedy_responses_labels_validation1800 --len_dataset 16479 --num_folds 1 --using_act layer --token answer_last --method transformer2_supcon_hallu_pos --bs 270 --epochs 300 --lr 0.00005 --norm_input True --save_probes True --save_path ~/Desktop/honest_llama_data --fast_mode True --plot_name nq-transformer_supcon-n*aug --tag ood --num_samples 9 --no_batch_sampling True

## Main ##
python get_activations_and_probe_non_linear_supcon_bce.py hl_llama_7B strqa --train_file_name strqa_baseline_responses_train --test_file_name strqa_baseline_responses_test --len_dataset 1832 --num_folds 1 --using_act layer --token answer_last --method individual_linear_hallu_pos --bs 128 --epochs 50 --lr 0.05 --norm_input True --save_probes True --save_path ~/Desktop/honest_llama_data --fast_mode True --plot_name linear-n-0.05 --tag main
python get_activations_and_probe_non_linear_supcon_bce.py hl_llama_7B strqa --train_file_name strqa_sampledplus_responses_train --test_file_name strqa_baseline_responses_test --len_dataset 16479 --num_folds 1 --using_act layer --token answer_last --method individual_linear_hallu_pos --bs 128 --epochs 50 --lr 0.05 --norm_input True --save_probes True --save_path ~/Desktop/honest_llama_data --fast_mode True --plot_name linear-n*aug-0.05 --tag main --num_samples 9
# python get_activations_and_probe_non_linear_supcon_bce.py hl_llama_7B strqa --train_file_name strqa_baseline_responses_train --test_file_name strqa_baseline_responses_test --len_dataset 1832 --num_folds 1 --using_act layer --token answer_last --method individual_non_linear_2_supcon_hallu_pos --bs 128 --epochs 300 --lr 0.0005 --norm_input True --save_probes True --save_path ~/Desktop/honest_llama_data --fast_mode True --plot_name linear_supcon-n --tag always_norm_input
# python get_activations_and_probe_non_linear_supcon_bce.py hl_llama_7B strqa --train_file_name strqa_sampledplus_responses_train --test_file_name strqa_baseline_responses_test --len_dataset 16479 --num_folds 1 --using_act layer --token answer_last --method individual_non_linear_2_supcon_hallu_pos --bs 180 --epochs 300 --lr 0.0005 --norm_input True --save_probes True --save_path ~/Desktop/honest_llama_data --fast_mode True --plot_name linear_supcon-n*aug --tag always_norm_input --num_samples 9 --no_batch_sampling True #--multi_gpu True
python get_activations_and_probe_transformer.py hl_llama_7B strqa --train_file_name strqa_baseline_responses_train --test_file_name strqa_baseline_responses_test --len_dataset 1832 --num_folds 1 --using_act layer --token answer_last --method transformer2_hallu_pos --bs 128 --epochs 50 --lr 0.00005 --norm_input True --save_probes True --save_path ~/Desktop/honest_llama_data --fast_mode True --plot_name transformer-n-0.00005 --tag main
python get_activations_and_probe_transformer.py hl_llama_7B strqa --train_file_name strqa_sampledplus_responses_train --test_file_name strqa_baseline_responses_test --len_dataset 16479 --num_folds 1 --using_act layer --token answer_last --method transformer2_hallu_pos --bs 128 --epochs 50 --lr 0.00005 --norm_input True --save_probes True --save_path ~/Desktop/honest_llama_data --fast_mode True --plot_name transformer-n*aug-0.00005 --tag main --num_samples 9
python get_activations_and_probe_transformer.py hl_llama_7B strqa --train_file_name strqa_sampledplus_responses_train --test_file_name strqa_baseline_responses_test --len_dataset 16479 --num_folds 1 --using_act layer --token answer_last --method transformer2_hallu_pos --bs 128 --epochs 50 --lr 0.00005 --norm_input True --save_probes True --save_path ~/Desktop/honest_llama_data --fast_mode True --plot_name transformer-cur-n*aug-0.00005 --tag main --num_samples 9 --retrain_model_path T_hl_llama_7B_strqa_baseline_responses_train_1832_1_layer_answer_last_transformer2_hallu_pos_bs128_epochs50_5e-05_False
# python get_activations_and_probe_transformer.py hl_llama_7B strqa --train_file_name strqa_baseline_responses_train --test_file_name strqa_baseline_responses_test --len_dataset 1832 --num_folds 1 --using_act layer --token answer_last --method transformer_supcon_hallu_pos --bs 128 --epochs 300 --lr 0.0005 --norm_input True --use_pe True --save_probes True --save_path ~/Desktop/honest_llama_data --fast_mode True --plot_name transformer_supcon-n --tag always_norm_input
python get_activations_and_probe_transformer.py hl_llama_7B strqa --train_file_name strqa_sampledplus_responses_train --test_file_name strqa_baseline_responses_test --len_dataset 16479 --num_folds 1 --using_act layer --token answer_last --method transformer2_supcon_hallu_pos --bs 270 --epochs 300 --lr 0.00005 --norm_input True --save_probes True --save_path ~/Desktop/honest_llama_data --fast_mode True --plot_name transformer_supcon-n*aug-0.00005 --tag main --num_samples 9 --no_batch_sampling True #--multi_gpu True
python get_activations_and_probe_transformer.py hl_llama_7B strqa --train_file_name strqa_sampledplus_responses_train --test_file_name strqa_baseline_responses_test --len_dataset 16479 --num_folds 1 --using_act layer --token answer_last --method transformer2_supcon_hallu_pos --bs 270 --epochs 300 --lr 0.00005 --norm_input True --save_probes True --save_path ~/Desktop/honest_llama_data --fast_mode True --plot_name transformer_supcon-cur-n*aug-0.00005 --tag main --num_samples 9 --no_batch_sampling True --retrain_model_path T_hl_llama_7B_strqa_baseline_responses_train_1832_1_layer_answer_last_transformer2_hallu_pos_bs128_epochs50_5e-05_False

## Analysis - Transformer Supcon ##
python get_activations_and_probe_transformer.py hl_llama_7B strqa --train_file_name strqa_sampledplus_responses_train --test_file_name strqa_baseline_responses_test --len_dataset 16479 --num_folds 1 --using_act layer --token answer_last --method transformer2_supcon_hallu_pos --bs 128 --epochs 300 --lr 0.00005 --norm_input True --save_probes True --save_path ~/Desktop/honest_llama_data --fast_mode True --plot_name transformer_supcon-n*aug-bs128 --tag analysis_transformer_supcon --num_samples 9

##########################################################################

## Hl-Llama-7B GSM8K ##

## Main ##
python get_activations_and_probe_non_linear_supcon_bce.py hl_llama_7B gsm8k --train_file_name gsm8k_greedy_responses_train2000 --test_file_name gsm8k_greedy_responses_test1800 --train_labels_file_name gsm8k_greedy_responses_labels_train2000 --test_labels_file_name gsm8k_greedy_responses_labels_test1800 --len_dataset 1999 --num_folds 1 --using_act layer --token answer_last --method individual_linear_hallu_pos --bs 128 --epochs 50 --lr 0.05 --norm_input True --save_probes True --save_path ~/honest_llama_data --fast_mode True  --plot_name linear-n-0.05b --tag main --use_best_val_t True
..
python get_activations_and_probe_transformer.py hl_llama_7B gsm8k --train_file_name gsm8k_greedy_responses_train2000 --train_labels_file_name gsm8k_greedy_responses_labels_train2000 --test_file_name gsm8k_greedy_responses_test1800 --test_labels_file_name gsm8k_greedy_responses_labels_test1800 --len_dataset 1999 --num_folds 1 --using_act layer --token answer_last --method transformer2_hallu_pos --bs 128 --epochs 50 --lr 0.00005 --norm_input True --save_probes True --save_path ~/honest_llama_data --fast_mode True --plot_name transformer-n-0.00005b --tag main --use_best_val_t True

##########################################################################

## Alpaca-7B Trivia QA ##
## Ablations - baseline probes##
python get_activations_and_probe_non_linear_supcon_bce.py alpaca_7B trivia_qa --train_file_name trivia_qa_greedy_responses_train5000 --test_file_name trivia_qa_greedy_responses_validation1800 --train_labels_file_name trivia_qa_greedy_responses_labels_train5000 --test_labels_file_name trivia_qa_greedy_responses_labels_validation1800 --len_dataset 2000 --num_folds 1 --using_act layer --token answer_last --method individual_linear_hallu_pos --bs 128 --epochs 50 --lr 0.0005 --save_probes True --save_path ~/Desktop/honest_llama_data --fast_mode True  --plot_name linear-n-raw-0.0005 --tag ablation
python get_activations_and_probe_non_linear_supcon_bce.py alpaca_7B trivia_qa --train_file_name trivia_qa_greedy_responses_train5000 --test_file_name trivia_qa_greedy_responses_validation1800 --train_labels_file_name trivia_qa_greedy_responses_labels_train5000 --test_labels_file_name trivia_qa_greedy_responses_labels_validation1800 --len_dataset 2000 --num_folds 1 --using_act layer --token answer_last --method individual_linear_hallu_pos --bs 128 --epochs 50 --lr 0.05 --norm_input True --save_probes True --save_path ~/Desktop/honest_llama_data --fast_mode True  --plot_name linear-n-norm-0.05 --tag ablation
python get_activations_and_probe_non_linear_supcon_bce.py alpaca_7B trivia_qa --train_file_name trivia_qa_greedy_responses_train5000 --test_file_name trivia_qa_greedy_responses_validation1800 --train_labels_file_name trivia_qa_greedy_responses_labels_train5000 --test_labels_file_name trivia_qa_greedy_responses_labels_validation1800 --len_dataset 2000 --num_folds 1 --using_act layer --token answer_last --method individual_linear_hallu_pos --bs 128 --epochs 50 --lr 0.0005 --norm_input True --save_probes True --save_path ~/Desktop/honest_llama_data --fast_mode True  --plot_name linear-n-meannorm-0.0005 --tag ablation
python get_activations_and_probe_non_linear_supcon_bce.py alpaca_7B trivia_qa --train_file_name trivia_qa_greedy_responses_train5000 --test_file_name trivia_qa_greedy_responses_validation1800 --train_labels_file_name trivia_qa_greedy_responses_labels_train5000 --test_labels_file_name trivia_qa_greedy_responses_labels_validation1800 --len_dataset 2000 --num_folds 1 --using_act layer --token answer_last --method individual_non_linear_4_hallu_pos --bs 128 --epochs 50 --lr 0.00005 --save_probes True --save_path ~/Desktop/honest_llama_data --fast_mode True  --plot_name nlinear-n-0.00005b --tag ablation --use_best_val_t True
python get_activations_and_probe_non_linear_supcon_bce.py alpaca_7B trivia_qa --train_file_name trivia_qa_greedy_responses_train5000 --test_file_name trivia_qa_greedy_responses_validation1800 --train_labels_file_name trivia_qa_greedy_responses_labels_train5000 --test_labels_file_name trivia_qa_greedy_responses_labels_validation1800 --len_dataset 2000 --num_folds 1 --using_act layer --token answer_last --method individual_non_linear_4_hallu_pos --bs 128 --epochs 50 --lr 0.0005 --norm_input True --save_probes True --save_path ~/Desktop/honest_llama_data --fast_mode True  --plot_name nlinear-n-norm-0.0005b --tag ablation --use_best_val_t True

## Ablations - transformer probes##
python get_activations_and_probe_transformer.py alpaca_7B trivia_qa --train_file_name trivia_qa_greedy_responses_train5000 --train_labels_file_name trivia_qa_greedy_responses_labels_train5000 --test_file_name trivia_qa_greedy_responses_validation1800 --test_labels_file_name trivia_qa_greedy_responses_labels_validation1800 --len_dataset 2000 --num_folds 1 --using_act layer --token answer_last --method transformer_hallu_pos --bs 128 --epochs 50 --lr 0.00005 --save_probes True --save_path ~/Desktop/honest_llama_data --fast_mode True --plot_name transformer-n-raw-0.00005 --tag ablation
python get_activations_and_probe_transformer.py alpaca_7B trivia_qa --train_file_name trivia_qa_greedy_responses_train5000 --train_labels_file_name trivia_qa_greedy_responses_labels_train5000 --test_file_name trivia_qa_greedy_responses_validation1800 --test_labels_file_name trivia_qa_greedy_responses_labels_validation1800 --len_dataset 2000 --num_folds 1 --using_act layer --token answer_last --method transformer_hallu_pos --bs 128 --epochs 50 --lr 0.00005 --norm_input True --save_probes True --save_path ~/Desktop/honest_llama_data --fast_mode True --plot_name transformer-n-norm-0.00005 --tag ablation
python get_activations_and_probe_transformer.py alpaca_7B trivia_qa --train_file_name trivia_qa_greedy_responses_train5000 --train_labels_file_name trivia_qa_greedy_responses_labels_train5000 --test_file_name trivia_qa_greedy_responses_validation1800 --test_labels_file_name trivia_qa_greedy_responses_labels_validation1800 --len_dataset 2000 --num_folds 1 --using_act layer --token answer_last --method transformer_hallu_pos --bs 128 --epochs 50 --lr 0.00005 --norm_input True --save_probes True --save_path ~/Desktop/honest_llama_data --fast_mode True --plot_name transformer-n-meannorm-0.00005 --tag ablation
python get_activations_and_probe_transformer.py alpaca_7B trivia_qa --train_file_name trivia_qa_greedy_responses_train5000 --train_labels_file_name trivia_qa_greedy_responses_labels_train5000 --test_file_name trivia_qa_greedy_responses_validation1800 --test_labels_file_name trivia_qa_greedy_responses_labels_validation1800 --len_dataset 2000 --num_folds 1 --using_act layer --token answer_last --method transformer_hallu_pos --bs 128 --epochs 50 --lr 0.00005 --save_probes True --save_path ~/Desktop/honest_llama_data --fast_mode True --plot_name transformer-n-raw-rawemb-0.00005 --tag ablation
python get_activations_and_probe_transformer.py alpaca_7B trivia_qa --train_file_name trivia_qa_greedy_responses_train5000 --train_labels_file_name trivia_qa_greedy_responses_labels_train5000 --test_file_name trivia_qa_greedy_responses_validation1800 --test_labels_file_name trivia_qa_greedy_responses_labels_validation1800 --len_dataset 2000 --num_folds 1 --using_act layer --token answer_last --method transformer_hallu_pos --bs 128 --epochs 50 --lr 0.00005 --norm_input True --save_probes True --save_path ~/Desktop/honest_llama_data --fast_mode True --plot_name transformer-n-norm-rawemb-0.00005 --tag ablation
python get_activations_and_probe_transformer.py alpaca_7B trivia_qa --train_file_name trivia_qa_greedy_responses_train5000 --train_labels_file_name trivia_qa_greedy_responses_labels_train5000 --test_file_name trivia_qa_greedy_responses_validation1800 --test_labels_file_name trivia_qa_greedy_responses_labels_validation1800 --len_dataset 2000 --num_folds 1 --using_act layer --token answer_last --method transformer_hallu_pos --bs 128 --epochs 50 --lr 0.00005 --norm_input True --save_probes True --save_path ~/Desktop/honest_llama_data --fast_mode True --plot_name transformer-n-meannorm-rawemb-0.00005 --tag ablation
python get_activations_and_probe_transformer.py alpaca_7B trivia_qa --train_file_name trivia_qa_greedy_responses_train5000 --train_labels_file_name trivia_qa_greedy_responses_labels_train5000 --test_file_name trivia_qa_greedy_responses_validation1800 --test_labels_file_name trivia_qa_greedy_responses_labels_validation1800 --len_dataset 2000 --num_folds 1 --using_act layer --token answer_last --method transformer_hallu_pos --bs 128 --epochs 50 --lr 0.0005 --norm_input True --use_pe True --save_probes True --save_path ~/Desktop/honest_llama_data --fast_mode True --plot_name transformer-n-norm-rawemb-pe-0.0005 --tag ablation
python get_activations_and_probe_transformer.py alpaca_7B trivia_qa --train_file_name trivia_qa_greedy_responses_train5000 --train_labels_file_name trivia_qa_greedy_responses_labels_train5000 --test_file_name trivia_qa_greedy_responses_validation1800 --test_labels_file_name trivia_qa_greedy_responses_labels_validation1800 --len_dataset 2000 --num_folds 1 --using_act layer --token answer_last --method transformer2_hallu_pos --bs 128 --epochs 50 --lr 0.00005 --norm_input True --save_probes True --save_path ~/Desktop/honest_llama_data --fast_mode True --plot_name transformer2-n-norm-rawemb-0.00005 --tag ablation

##########################################################################

## Alpaca-7B NQ ##
## Ablations - baseline probes##
python get_activations_and_probe_non_linear_supcon_bce.py alpaca_7B nq_open --train_file_name nq_open_greedy_responses_train5000 --test_file_name nq_open_greedy_responses_validation1800 --train_labels_file_name nq_open_greedy_responses_labels_train5000 --test_labels_file_name nq_open_greedy_responses_labels_validation1800 --len_dataset 2000 --num_folds 1 --using_act layer --token answer_last --method individual_linear_hallu_pos --bs 128 --epochs 50 --lr 0.0005 --save_probes True --save_path ~/Desktop/honest_llama_data --fast_mode True  --plot_name linear-n-raw-0.0005 --tag ablation
python get_activations_and_probe_non_linear_supcon_bce.py alpaca_7B nq_open --train_file_name nq_open_greedy_responses_train5000 --test_file_name nq_open_greedy_responses_validation1800 --train_labels_file_name nq_open_greedy_responses_labels_train5000 --test_labels_file_name nq_open_greedy_responses_labels_validation1800 --len_dataset 2000 --num_folds 1 --using_act layer --token answer_last --method individual_linear_hallu_pos --bs 128 --epochs 50 --lr 0.05 --norm_input True --save_probes True --save_path ~/Desktop/honest_llama_data --fast_mode True  --plot_name linear-n-norm-0.05 --tag ablation
python get_activations_and_probe_non_linear_supcon_bce.py alpaca_7B nq_open --train_file_name nq_open_greedy_responses_train5000 --test_file_name nq_open_greedy_responses_validation1800 --train_labels_file_name nq_open_greedy_responses_labels_train5000 --test_labels_file_name nq_open_greedy_responses_labels_validation1800 --len_dataset 2000 --num_folds 1 --using_act layer --token answer_last --method individual_linear_hallu_pos --bs 128 --epochs 50 --lr 0.0005 --norm_input True --save_probes True --save_path ~/Desktop/honest_llama_data --fast_mode True  --plot_name linear-n-meannorm-0.0005 --tag ablation
python get_activations_and_probe_non_linear_supcon_bce.py alpaca_7B nq_open --train_file_name nq_open_greedy_responses_train5000 --test_file_name nq_open_greedy_responses_validation1800 --train_labels_file_name nq_open_greedy_responses_labels_train5000 --test_labels_file_name nq_open_greedy_responses_labels_validation1800 --len_dataset 2000 --num_folds 1 --using_act layer --token answer_last --method individual_non_linear_4_hallu_pos --bs 128 --epochs 50 --lr 0.00005 --save_probes True --save_path ~/Desktop/honest_llama_data --fast_mode True  --plot_name nlinear-n-0.00005b --tag ablation --use_best_val_t True
python get_activations_and_probe_non_linear_supcon_bce.py alpaca_7B nq_open --train_file_name nq_open_greedy_responses_train5000 --test_file_name nq_open_greedy_responses_validation1800 --train_labels_file_name nq_open_greedy_responses_labels_train5000 --test_labels_file_name nq_open_greedy_responses_labels_validation1800 --len_dataset 2000 --num_folds 1 --using_act layer --token answer_last --method individual_non_linear_4_hallu_pos --bs 128 --epochs 50 --lr 0.0005 --norm_input True --save_probes True --save_path ~/Desktop/honest_llama_data --fast_mode True  --plot_name nlinear-n-norm-0.0005b --tag ablation --use_best_val_t True

## Ablations - transformer probes##
python get_activations_and_probe_transformer.py alpaca_7B nq_open --train_file_name nq_open_greedy_responses_train5000 --train_labels_file_name nq_open_greedy_responses_labels_train5000 --test_file_name nq_open_greedy_responses_validation1800 --test_labels_file_name nq_open_greedy_responses_labels_validation1800 --len_dataset 2000 --num_folds 1 --using_act layer --token answer_last --method transformer_hallu_pos --bs 128 --epochs 50 --lr 0.0005 --save_probes True --save_path ~/Desktop/honest_llama_data --fast_mode True --plot_name transformer-n-raw-0.0005 --tag ablation
python get_activations_and_probe_transformer.py alpaca_7B nq_open --train_file_name nq_open_greedy_responses_train5000 --train_labels_file_name nq_open_greedy_responses_labels_train5000 --test_file_name nq_open_greedy_responses_validation1800 --test_labels_file_name nq_open_greedy_responses_labels_validation1800 --len_dataset 2000 --num_folds 1 --using_act layer --token answer_last --method transformer_hallu_pos --bs 128 --epochs 50 --lr 0.005 --norm_input True --save_probes True --save_path ~/Desktop/honest_llama_data --fast_mode True --plot_name transformer-n-norm-0.005 --tag ablation
python get_activations_and_probe_transformer.py alpaca_7B nq_open --train_file_name nq_open_greedy_responses_train5000 --train_labels_file_name nq_open_greedy_responses_labels_train5000 --test_file_name nq_open_greedy_responses_validation1800 --test_labels_file_name nq_open_greedy_responses_labels_validation1800 --len_dataset 2000 --num_folds 1 --using_act layer --token answer_last --method transformer_hallu_pos --bs 128 --epochs 50 --lr 0.05 --norm_input True --save_probes True --save_path ~/Desktop/honest_llama_data --fast_mode True --plot_name transformer-n-meannorm-0.05 --tag ablation
python get_activations_and_probe_transformer.py alpaca_7B nq_open --train_file_name nq_open_greedy_responses_train5000 --train_labels_file_name nq_open_greedy_responses_labels_train5000 --test_file_name nq_open_greedy_responses_validation1800 --test_labels_file_name nq_open_greedy_responses_labels_validation1800 --len_dataset 2000 --num_folds 1 --using_act layer --token answer_last --method transformer_hallu_pos --bs 128 --epochs 50 --lr 0.00005 --save_probes True --save_path ~/Desktop/honest_llama_data --fast_mode True --plot_name transformer-n-raw-rawemb-0.00005 --tag ablation
python get_activations_and_probe_transformer.py alpaca_7B nq_open --train_file_name nq_open_greedy_responses_train5000 --train_labels_file_name nq_open_greedy_responses_labels_train5000 --test_file_name nq_open_greedy_responses_validation1800 --test_labels_file_name nq_open_greedy_responses_labels_validation1800 --len_dataset 2000 --num_folds 1 --using_act layer --token answer_last --method transformer_hallu_pos --bs 128 --epochs 50 --lr 0.00005 --norm_input True --save_probes True --save_path ~/Desktop/honest_llama_data --fast_mode True --plot_name transformer-n-norm-rawemb-0.00005 --tag ablation
python get_activations_and_probe_transformer.py alpaca_7B nq_open --train_file_name nq_open_greedy_responses_train5000 --train_labels_file_name nq_open_greedy_responses_labels_train5000 --test_file_name nq_open_greedy_responses_validation1800 --test_labels_file_name nq_open_greedy_responses_labels_validation1800 --len_dataset 2000 --num_folds 1 --using_act layer --token answer_last --method transformer_hallu_pos --bs 128 --epochs 50 --lr 0.00005 --norm_input True --save_probes True --save_path ~/Desktop/honest_llama_data --fast_mode True --plot_name transformer-n-meannorm-rawemb-0.00005 --tag ablation
python get_activations_and_probe_transformer.py alpaca_7B nq_open --train_file_name nq_open_greedy_responses_train5000 --train_labels_file_name nq_open_greedy_responses_labels_train5000 --test_file_name nq_open_greedy_responses_validation1800 --test_labels_file_name nq_open_greedy_responses_labels_validation1800 --len_dataset 2000 --num_folds 1 --using_act layer --token answer_last --method transformer_hallu_pos --bs 128 --epochs 50 --lr 0.00005 --norm_input True --save_probes True --save_path ~/Desktop/honest_llama_data --fast_mode True --plot_name transformer-n-norm-rawemb-pe-0.00005 --tag ablation
python get_activations_and_probe_transformer.py alpaca_7B nq_open --train_file_name nq_open_greedy_responses_train5000 --train_labels_file_name nq_open_greedy_responses_labels_train5000 --test_file_name nq_open_greedy_responses_validation1800 --test_labels_file_name nq_open_greedy_responses_labels_validation1800 --len_dataset 2000 --num_folds 1 --using_act layer --token answer_last --method transformer2_hallu_pos --bs 128 --epochs 50 --lr 0.005 --norm_input True --save_probes True --save_path ~/Desktop/honest_llama_data --fast_mode True --plot_name transformer2-n-norm-rawemb-0.005 --tag ablation

## Additional - tokens ##
python get_activations_and_probe_transformer.py alpaca_7B nq_open --train_file_name nq_open_greedy_responses_train5000 --train_labels_file_name nq_open_greedy_responses_labels_train5000 --test_file_name nq_open_greedy_responses_validation1800 --test_labels_file_name nq_open_greedy_responses_labels_validation1800 --len_dataset 2000 --num_folds 1 --using_act layer --token tagged_tokens --method transformer2_hallu_pos --bs 32 --epochs 50 --lr 0.00005 --norm_input True --save_probes True --device 0 --save_path ~/Desktop/honest_llama_data --max_tokens 40 --use_pe True --plot_name transformer-n-tagged-0.00005 --tag additional_tokens

## Main ##
# python get_activations_and_probe_non_linear_supcon_bce.py alpaca_7B nq_open --train_file_name nq_open_greedy_responses_train5000 --test_file_name nq_open_greedy_responses_validation1800 --train_labels_file_name nq_open_greedy_responses_labels_train5000 --test_labels_file_name nq_open_greedy_responses_labels_validation1800 --len_dataset 2000 --num_folds 1 --using_act layer --token answer_last --method individual_linear_hallu_pos --bs 128 --epochs 50 --lr 0.05 --norm_input True --save_probes True --save_path ~/Desktop/honest_llama_data --fast_mode True  --plot_name linear-n-0.05 --tag main
# python get_activations_and_probe_non_linear_supcon_bce.py alpaca_7B nq_open --train_file_name nq_open_sampledplus_responses_train2000 --test_file_name nq_open_greedy_responses_validation1800 --train_labels_file_name nq_open_sampledplus_responses_labels_train2000 --test_labels_file_name nq_open_greedy_responses_labels_validation1800 --len_dataset 22000 --num_folds 1 --using_act layer --token answer_last --method individual_linear_hallu_pos --bs 128 --epochs 50 --lr 0.05 --norm_input True --save_probes True --save_path ~/Desktop/honest_llama_data --fast_mode True  --plot_name linear-n*aug-0.05 --tag main --num_samples 11
# python get_activations_and_probe_transformer.py alpaca_7B nq_open --train_file_name nq_open_greedy_responses_train5000 --train_labels_file_name nq_open_greedy_responses_labels_train5000 --test_file_name nq_open_greedy_responses_validation1800 --test_labels_file_name nq_open_greedy_responses_labels_validation1800 --len_dataset 2000 --num_folds 1 --using_act layer --token answer_last --method transformer2_hallu_pos --bs 128 --epochs 50 --lr 0.00005 --norm_input True --save_probes True --save_path ~/Desktop/honest_llama_data --fast_mode True --plot_name transformer-n-0.00005 --tag main

##########################################################################