# python get_activations.py alpaca_7B nq_open --token tagged_tokens --file_name nq_open_greedy_responses_train5000  --device 0 --save_path ~/Desktop/honest_llama_data
# python get_activations.py alpaca_7B nq_open --token tagged_tokens --file_name nq_open_greedy_responses_validation1800  --device 0 --save_path ~/Desktop/honest_llama_data
# python get_activations_and_probe_transformer.py alpaca_7B nq_open --train_file_name nq_open_greedy_responses_train5000 --train_labels_file_name nq_open_greedy_responses_labels_train5000 --test_file_name nq_open_greedy_responses_validation1800 --test_labels_file_name nq_open_greedy_responses_labels_validation1800 --len_dataset 5000 --num_folds 1 --using_act layer --token tagged_tokens --method transformer_hallu_pos --bs 32 --epochs 50 --lr 0.0001 --norm_input True --save_probes True --device 0 --save_path ~/Desktop/honest_llama_data --max_tokens 40 --use_pe True


python get_activations.py hl_llama_7B strqa --token answer_last --file_name strqa_sampledplus_responses_train  --device 0 --save_path ~/honest_llama_data
python get_activations.py hl_llama_7B strqa --token answer_last --file_name strqa_sampledplus_responses_test  --device 0 --save_path ~/honest_llama_data

python get_activations_and_probe_non_linear_supcon_bce.py hl_llama_7B strqa --train_file_name strqa_baseline_responses_train --test_file_name strqa_baseline_responses_test --len_dataset 1832 --num_folds 1 --using_act layer --token answer_last --method individual_linear_hallu_pos --bs 128 --epochs 300 --lr 0.005 --norm_input True --save_probes True --save_path ~/Desktop/honest_llama_data --fast_mode True --plot_name linear-n --tag always_norm_input
python get_activations_and_probe_non_linear_supcon_bce.py hl_llama_7B strqa --train_file_name strqa_sampledplus_responses_train --test_file_name strqa_baseline_responses_test --len_dataset 16479 --num_folds 1 --using_act layer --token answer_last --method individual_linear_hallu_pos --bs 128 --epochs 300 --lr 0.005 --norm_input True --save_probes True --save_path ~/Desktop/honest_llama_data --fast_mode True --plot_name linear-n*aug --tag always_norm_input --num_samples 9 #--multi_gpu True

python get_activations_and_probe_non_linear_supcon_bce.py hl_llama_7B strqa --train_file_name strqa_baseline_responses_train --test_file_name strqa_baseline_responses_test --len_dataset 1832 --num_folds 1 --using_act layer --token answer_last --method individual_non_linear_2_supcon_hallu_pos --bs 128 --epochs 300 --lr 0.0005 --norm_input True --save_probes True --save_path ~/Desktop/honest_llama_data --fast_mode True --plot_name linear_supcon-n --tag always_norm_input
python get_activations_and_probe_non_linear_supcon_bce.py hl_llama_7B strqa --train_file_name strqa_sampledplus_responses_train --test_file_name strqa_baseline_responses_test --len_dataset 16479 --num_folds 1 --using_act layer --token answer_last --method individual_non_linear_2_supcon_hallu_pos --bs 180 --epochs 300 --lr 0.0005 --norm_input True --save_probes True --save_path ~/Desktop/honest_llama_data --fast_mode True --plot_name linear_supcon-n*aug --tag always_norm_input --num_samples 9 --no_batch_sampling True #--multi_gpu True

python get_activations_and_probe_transformer.py hl_llama_7B strqa --train_file_name strqa_baseline_responses_train --test_file_name strqa_baseline_responses_test --len_dataset 1832 --num_folds 1 --using_act layer --token answer_last --method transformer_hallu_pos --bs 128 --epochs 300 --lr 0.0005 --norm_input True --use_pe True --save_probes True --save_path ~/Desktop/honest_llama_data --fast_mode True --plot_name transformer-n --tag always_norm_input
python get_activations_and_probe_transformer.py hl_llama_7B strqa --train_file_name strqa_sampledplus_responses_train --test_file_name strqa_baseline_responses_test --len_dataset 16479 --num_folds 1 --using_act layer --token answer_last --method transformer_hallu_pos --bs 128 --epochs 300 --lr 0.0005 --norm_input True --use_pe True --save_probes True --save_path ~/Desktop/honest_llama_data --fast_mode True --plot_name transformer-n*aug --tag always_norm_input --num_samples 9 #--multi_gpu True

python get_activations_and_probe_transformer.py hl_llama_7B strqa --train_file_name strqa_baseline_responses_train --test_file_name strqa_baseline_responses_test --len_dataset 1832 --num_folds 1 --using_act layer --token answer_last --method transformer_supcon_hallu_pos --bs 128 --epochs 300 --lr 0.0005 --norm_input True --use_pe True --save_probes True --save_path ~/Desktop/honest_llama_data --fast_mode True --plot_name transformer_supcon-n --tag always_norm_input
python get_activations_and_probe_transformer.py hl_llama_7B strqa --train_file_name strqa_sampledplus_responses_train --test_file_name strqa_baseline_responses_test --len_dataset 16479 --num_folds 1 --using_act layer --token answer_last --method transformer_supcon_hallu_pos --bs 180 --epochs 300 --lr 0.0005 --norm_input True --use_pe True --save_probes True --save_path ~/Desktop/honest_llama_data --fast_mode True --plot_name transformer_supcon-n*aug --tag always_norm_input --num_samples 9 --no_batch_sampling True #--multi_gpu True