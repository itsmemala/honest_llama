## Gen data
accelerate launch --num_processes 2 --multi-gpu strqa_eval.py --model-name circulus/alpaca-7b --early-exit-layers 0,2,4,6,8,10,12,14,32 --data-path /home/local/data/ms/honest_llama_data --test_data_file /home/local/data/ms/honest_llama_data/responses/alpaca_7B_strqa_baseline_responses_test.json --output-path /home/local/data/ms/honest_llama_data/responses/alpaca_7B_strqa_dola_0to16_responses_test.json --num-gpus 2
accelerate launch --num_processes 2 --multi-gpu trivia_qa_eval.py --model-name circulus/alpaca-7b --early-exit-layers 16,18,20,22,24,26,28,30,32 --data-path /home/local/data/ms/honest_llama_data --output-path /home/local/data/ms/honest_llama_data/responses/alpaca_7B_trivia_qa_dola16to32_responses_test.json --num-gpus 2


#### Pred on dola using probes
Trivia:
dola+clap II (CLAP-g): python get_activations_and_probe_transformer.py hl_llama_7B trivia_qa --train_file_name trivia_qa_greedy_responses_train5000 --test_file_name trivia_qa_dola16to32_responses_test --train_labels_file_name trivia_qa_greedy_responses_labels_train5000 --len_dataset 5000 --num_folds 1 --using_act layer --token answer_last --method transformer_hallu_pos --bs 128 --epochs 50 --lr_list 0.00005,0.0005,0.005 --save_probes True --save_path /home/local/data/ms/honest_llama_data --fast_mode True  --use_best_val_t True --seed_list 42,101,2650 --best_using_auc True  --test_num_samples 1 --ood_test True  --skip_train True;
dola+clap II (CLAP-wp): python get_activations_and_probe_transformer.py hl_llama_7B trivia_qa --train_file_name trivia_qa_sampledplus_responses_train5000 --test_file_name trivia_qa_dola16to32_responses_test --train_labels_file_name trivia_qa_sampledplus_responses_labels_train5000 --len_dataset 55000 --num_folds 1 --using_act layer --token answer_last --method transformer_hallu_pos --bs 352 --no_batch_sampling True --epochs 50 --lr_list 0.00005,0.0005,0.005 --save_probes True --save_path /home/local/data/ms/honest_llama_data --fast_mode True  --use_best_val_t True --seed_list 42,101,2650 --best_using_auc True  --test_num_samples 1 --ood_test True  --skip_train True;

Str:
dola+clap II (CLAP-g): python get_activations_and_probe_transformer.py hl_llama_7B strqa --train_file_name strqa_baseline_responses_train --test_file_name strqa_dola_0to16_responses_test --len_dataset 1832 --num_folds 1 --using_act layer --token answer_last --method transformer_hallu_pos --bs 128 --epochs 50 --lr_list 0.00005,0.0005,0.005 --save_probes True --save_path /home/local/data/ms/honest_llama_data --fast_mode True  --use_best_val_t True --seed_list 42,101,2650 --best_using_auc True  --test_num_samples 1 --ood_test True  --skip_train True;
dola+clap II (CLAP-wp): python get_activations_and_probe_transformer.py hl_llama_7B strqa --train_file_name strqa_sampledplus_responses_train --test_file_name strqa_dola_0to16_responses_test --len_dataset 16479 --num_folds 1 --using_act layer --token answer_last --method transformer_hallu_pos --bs 288 --no_batch_sampling True --epochs 50 --lr_list 0.00005,0.0005,0.005 --save_probes True --save_path /home/local/data/ms/honest_llama_data --fast_mode True  --use_best_val_t True --seed_list 42,101,2650 --best_using_auc True  --test_num_samples 1 --ood_test True  --skip_train True;

Alp-Trivia: match by full_input_text
dola+clap II (CLAP-wp): python get_activations_and_probe_transformer.py alpaca_7B trivia_qa --train_file_name trivia_qa_sampledplus_responses_train5000 --test_file_name trivia_qa_dola16to32_responses_test --train_labels_file_name trivia_qa_sampledplus_responses_labels_train5000 --len_dataset 55000 --num_folds 1 --using_act layer --token answer_last --method transformer_hallu_pos --bs 352 --no_batch_sampling True --epochs 50 --lr_list 0.00005,0.0005,0.005 --save_probes True --save_path /home/local/data/ms/honest_llama_data --fast_mode True  --use_best_val_t True --seed_list 42,101,2650 --best_using_auc True  --test_num_samples 1 --ood_test True  --skip_train True;

Alp-Str: match by full_input_text
dola+clap II (CLAP-wp): python get_activations_and_probe_transformer.py alpaca_7B strqa --train_file_name strqa_sampledplus_responses_train --test_file_name strqa_dola_0to16_responses_test --len_dataset 16479 --num_folds 1 --using_act layer --token answer_last --method transformer_hallu_pos --bs 288 --no_batch_sampling True --epochs 50 --lr_list 0.00005,0.0005,0.005 --save_probes True --save_path /home/local/data/ms/honest_llama_data --fast_mode True  --use_best_val_t True --seed_list 42,101,2650 --best_using_auc True  --test_num_samples 1 --ood_test True  --skip_train True;

## Results ##########
Trivia:
dola+LP I: python analyse_crossdataset_bce.py hl_llama_7B trivia_qa --using_act layer --token answer_last --probes_file_name NLSC42_/hl_llama_7B_/trivia_qa_greedy_responses_train5000_/5000_1_layerFalse_answer_last_individual_linear_hallu_pos_bs128_epochs50_ --probes_file_name_concat ba --lr_list 0.00005,0.0005,0.005 --best_threshold True --save_path /home/local/data/ms/honest_llama_data --seed_list 42,101,2650 --fpr_at_recall -1 --mitigated_responses_file_name dola16to32_responses_test
dola+clap I: python analyse_crossdataset_bce_transfmr.py hl_llama_7B trivia_qa --using_act layer --token answer_last --probes_file_name T42_/hl_llama_7B_/trivia_qa_greedy_responses_train5000_/5000_1_layerFalse_answer_last_transformer_hallu_pos_bs128_epochs50_ --probes_file_name_concat ba --lr_list 0.00005,0.0005,0.005 --best_threshold True --save_path /home/local/data/ms/honest_llama_data --seed_list 42,101,2650 --fpr_at_recall -1 --mitigated_responses_file_name dola16to32_responses_test
dola+clap II (CLAP-g) (get probes name): python analyse_crossdataset_bce_transfmr.py hl_llama_7B trivia_qa --using_act layer --token answer_last --probes_file_name T42_/hl_llama_7B_/ood_trivia_qa/_trivia_qa_dola16to32_responses_test_5000_1_layerFalse_answer_last_transformer_hallu_pos_bs128_epochs50_ --probes_file_name_concat ba --lr_list 0.00005,0.0005,0.005 --best_threshold True --save_path /home/local/data/ms/honest_llama_data --seed_list 42,101,2650 --fpr_at_recall -1
                                AUC: 81.4 (0.3)
dola+clap II (CLAP-g): python analyse_crossdataset_bce_transfmr.py hl_llama_7B trivia_qa --using_act layer --token answer_last --probes_file_name T42_/hl_llama_7B_/trivia_qa_greedy_responses_train5000_/5000_1_layerFalse_answer_last_transformer_hallu_pos_bs128_epochs50_ --probes_file_name_concat ba --lr_list 0.00005,0.0005,0.005 --best_threshold True --save_path /home/local/data/ms/honest_llama_data --seed_list 42,101,2650 --fpr_at_recall -1 --mitigated_responses_file_name dola16to32_responses_test --m_probes_file_name T42_/hl_llama_7B_/ood_trivia_qa/_trivia_qa_dola16to32_responses_test_5000_1_layerFalse_answer_last_transformer_hallu_pos_bs128_epochs50_5e-05_Falseba,T101_/hl_llama_7B_/ood_trivia_qa/_trivia_qa_dola16to32_responses_test_5000_1_layerFalse_answer_last_transformer_hallu_pos_bs128_epochs50_0.0005_Falseba,T2650_/hl_llama_7B_/ood_trivia_qa/_trivia_qa_dola16to32_responses_test_5000_1_layerFalse_answer_last_transformer_hallu_pos_bs128_epochs50_5e-05_Falseba
dola+clap II (CLAP-wp) (get probes name): python analyse_crossdataset_bce_transfmr.py hl_llama_7B trivia_qa --using_act layer --token answer_last --probes_file_name T42_/hl_llama_7B_/ood_trivia_qa/_trivia_qa_dola16to32_responses_test_55000_1_layerFalse_answer_last_transformer_hallu_pos_bs352_epochs50_ --probes_file_name_concat ba --lr_list 0.00005,0.0005,0.005 --best_threshold True --save_path /home/local/data/ms/honest_llama_data --seed_list 42,101,2650 --fpr_at_recall -1
                                AUC: 83.9 (0.8)
dola+clap II (CLAP-wp): python analyse_crossdataset_bce_transfmr.py hl_llama_7B trivia_qa --using_act layer --token answer_last --probes_file_name T42_/hl_llama_7B_/trivia_qa_greedy_responses_train5000_/5000_1_layerFalse_answer_last_transformer_hallu_pos_bs128_epochs50_ --probes_file_name_concat ba --lr_list 0.00005,0.0005,0.005 --best_threshold True --save_path /home/local/data/ms/honest_llama_data --seed_list 42,101,2650 --fpr_at_recall -1 --mitigated_responses_file_name dola16to32_responses_test --m_probes_file_name T42_/hl_llama_7B_/ood_trivia_qa/_trivia_qa_dola16to32_responses_test_55000_1_layerFalse_answer_last_transformer_hallu_pos_bs352_epochs50_0.005_Falseba,T101_/hl_llama_7B_/ood_trivia_qa/_trivia_qa_dola16to32_responses_test_55000_1_layerFalse_answer_last_transformer_hallu_pos_bs352_epochs50_5e-05_Falseba,T2650_/hl_llama_7B_/ood_trivia_qa/_trivia_qa_dola16to32_responses_test_55000_1_layerFalse_answer_last_transformer_hallu_pos_bs352_epochs50_5e-05_Falseba

sampled+clap I: python analyse_crossdataset_bce_transfmr.py hl_llama_7B trivia_qa --using_act layer --token answer_last --probes_file_name T42_/hl_llama_7B_/trivia_qa_greedy_responses_train5000_/5000_1_layerFalse_answer_last_transformer_hallu_pos_bs128_epochs50_ --probes_file_name_concat ba --lr_list 0.00005,0.0005,0.005 --best_threshold True --save_path /home/local/data/ms/honest_llama_data --seed_list 42,101,2650 --fpr_at_recall -1 --mitigated_responses_file_name sampled_responses_labels_validation1800
sampled+clap II (CLAP-wp) (get probes name): python analyse_crossdataset_bce_transfmr.py hl_llama_7B trivia_qa --using_act layer --token answer_last --probes_file_name T42_/hl_llama_7B_/trivia_qa_sampledplus_responses_train5000_trivia_qa_sampled_responses_validation1800_/55000_1_layerFalse_answer_last_transformer2_hallu_pos_bs352_epochs50_ --probes_file_name_concat ba --lr_list 0.00005,0.0005,0.005 --best_threshold True --save_path /home/local/data/ms/honest_llama_data --seed_list 42,101,2650 --fpr_at_recall -1 --test_num_samples 10
sampled+clap II (CLAP-wp): python analyse_crossdataset_bce_transfmr.py hl_llama_7B trivia_qa --using_act layer --token answer_last --probes_file_name T42_/hl_llama_7B_/trivia_qa_greedy_responses_train5000_/5000_1_layerFalse_answer_last_transformer_hallu_pos_bs128_epochs50_ --probes_file_name_concat ba --lr_list 0.00005,0.0005,0.005 --best_threshold True --save_path /home/local/data/ms/honest_llama_data --seed_list 42,101,2650 --fpr_at_recall -1 --mitigated_responses_file_name sampled_responses_labels_validation1800 --test_num_samples 10 --m_probes_file_name T42_/hl_llama_7B_/trivia_qa_sampledplus_responses_train5000_trivia_qa_sampled_responses_validation1800_/55000_1_layerFalse_answer_last_transformer2_hallu_pos_bs352_epochs50_0.0005_Falseba,T101_/hl_llama_7B_/trivia_qa_sampledplus_responses_train5000_trivia_qa_sampled_responses_validation1800_/55000_1_layerFalse_answer_last_transformer2_hallu_pos_bs352_epochs50_0.0005_Falseba,T2650_/hl_llama_7B_/trivia_qa_sampledplus_responses_train5000_trivia_qa_sampled_responses_validation1800_/55000_1_layerFalse_answer_last_transformer2_hallu_pos_bs352_epochs50_5e-05_Falseba

Str:
dola+LP I: python analyse_crossdataset_bce.py hl_llama_7B strqa --using_act layer --token answer_last --probes_file_name NLSC42_/hl_llama_7B_/strqa_baseline_responses_train_/1832_1_layerFalse_answer_last_individual_linear_hallu_pos_bs128_epochs50_ --lr_list 0.00005,0.0005,0.005,0.05,0.5 --best_threshold True --save_path ~/Desktop/honest_llama_data --seed_list 42,101,2650 --fpr_at_recall -1 --mitigated_responses_file_name dola_0to16_responses_test
dola+clap I: python analyse_crossdataset_bce_transfmr.py hl_llama_7B strqa --using_act layer --token answer_last --probes_file_name T42_/hl_llama_7B_/strqa_baseline_responses_train_/1832_1_layerFalse_answer_last_transformer_hallu_pos_bs128_epochs50_ --probes_file_name_concat ba --lr_list 0.00005,0.0005,0.005 --best_threshold True --save_path /home/local/data/ms/honest_llama_data --seed_list 42,101,2650 --fpr_at_recall -1 --mitigated_responses_file_name dola_0to16_responses_test
dola+clap II (CLAP-g) (get probes name): python analyse_crossdataset_bce_transfmr.py hl_llama_7B strqa --using_act layer --token answer_last --probes_file_name T42_/hl_llama_7B_/ood_strqa/_strqa_dola_0to16_responses_test_1832_1_layerFalse_answer_last_transformer_hallu_pos_bs128_epochs50_ --probes_file_name_concat ba --lr_list 0.00005,0.0005,0.005 --best_threshold True --save_path /home/local/data/ms/honest_llama_data --seed_list 42,101,2650 --fpr_at_recall -1
                                AUC: 62.1 (0.1)
dola+clap II (CLAP-g): python analyse_crossdataset_bce_transfmr.py hl_llama_7B strqa --using_act layer --token answer_last --probes_file_name T42_/hl_llama_7B_/strqa_baseline_responses_train_/1832_1_layerFalse_answer_last_transformer_hallu_pos_bs128_epochs50_ --probes_file_name_concat ba --lr_list 0.00005,0.0005,0.005 --best_threshold True --save_path /home/local/data/ms/honest_llama_data --seed_list 42,101,2650 --fpr_at_recall -1 --mitigated_responses_file_name dola_0to16_responses_test --m_probes_file_name T42_/hl_llama_7B_/ood_strqa/_strqa_dola_0to16_responses_test_1832_1_layerFalse_answer_last_transformer_hallu_pos_bs128_epochs50_5e-05_Falseba,T101_/hl_llama_7B_/ood_strqa/_strqa_dola_0to16_responses_test_1832_1_layerFalse_answer_last_transformer_hallu_pos_bs128_epochs50_0.0005_Falseba,T2650_/hl_llama_7B_/ood_strqa/_strqa_dola_0to16_responses_test_1832_1_layerFalse_answer_last_transformer_hallu_pos_bs128_epochs50_0.0005_Falseba
dola+clap II (CLAP-wp) (get probes name): python analyse_crossdataset_bce_transfmr.py hl_llama_7B strqa --using_act layer --token answer_last --probes_file_name T42_/hl_llama_7B_/ood_strqa/_strqa_dola_0to16_responses_test_16479_1_layerFalse_answer_last_transformer_hallu_pos_bs288_epochs50_ --probes_file_name_concat ba --lr_list 0.00005,0.0005,0.005 --best_threshold True --save_path /home/local/data/ms/honest_llama_data --seed_list 42,101,2650 --fpr_at_recall -1
                                AUC: 64.8 (1.0)
dola+clap II (CLAP-wp): python analyse_crossdataset_bce_transfmr.py hl_llama_7B strqa --using_act layer --token answer_last --probes_file_name T42_/hl_llama_7B_/strqa_baseline_responses_train_/1832_1_layerFalse_answer_last_transformer_hallu_pos_bs128_epochs50_ --probes_file_name_concat ba --lr_list 0.00005,0.0005,0.005 --best_threshold True --save_path /home/local/data/ms/honest_llama_data --seed_list 42,101,2650 --fpr_at_recall -1 --mitigated_responses_file_name dola_0to16_responses_test --m_probes_file_name T42_/hl_llama_7B_/ood_strqa/_strqa_dola_0to16_responses_test_16479_1_layerFalse_answer_last_transformer_hallu_pos_bs288_epochs50_0.005_Falseba,T101_/hl_llama_7B_/ood_strqa/_strqa_dola_0to16_responses_test_16479_1_layerFalse_answer_last_transformer_hallu_pos_bs288_epochs50_5e-05_Falseba,T2650_/hl_llama_7B_/ood_strqa/_strqa_dola_0to16_responses_test_16479_1_layerFalse_answer_last_transformer_hallu_pos_bs288_epochs50_5e-05_Falseba

sampled+clap I: python analyse_crossdataset_bce_transfmr.py hl_llama_7B strqa --using_act layer --token answer_last --probes_file_name T42_/hl_llama_7B_/strqa_baseline_responses_train_/1832_1_layerFalse_answer_last_transformer_hallu_pos_bs128_epochs50_ --probes_file_name_concat ba --lr_list 0.00005,0.0005,0.005 --best_threshold True --save_path /home/local/data/ms/honest_llama_data --seed_list 42,101,2650 --fpr_at_recall -1 --mitigated_responses_file_name sampled_responses_test
sampled+clap II (CLAP-wp) (get probes name): python analyse_crossdataset_bce_transfmr.py hl_llama_7B strqa --using_act layer --token answer_last --probes_file_name T42_/hl_llama_7B_/strqa_sampledplus_responses_train_strqa_sampled_responses_test_/16479_1_layerFalse_answer_last_transformer2_hallu_pos_bs288_epochs50_ --probes_file_name_concat ba --lr_list 0.00005,0.0005,0.005 --best_threshold True --save_path /home/local/data/ms/honest_llama_data --seed_list 42,101,2650 --fpr_at_recall -1 --test_num_samples 8
sampled+clap II (CLAP-wp): python analyse_crossdataset_bce_transfmr.py hl_llama_7B strqa --using_act layer --token answer_last --probes_file_name T42_/hl_llama_7B_/strqa_baseline_responses_train_/1832_1_layerFalse_answer_last_transformer_hallu_pos_bs128_epochs50_ --probes_file_name_concat ba --lr_list 0.00005,0.0005,0.005 --best_threshold True --save_path /home/local/data/ms/honest_llama_data --seed_list 42,101,2650 --fpr_at_recall -1 --mitigated_responses_file_name sampled_responses_test --test_num_samples 8 --m_probes_file_name T42_/hl_llama_7B_/strqa_sampledplus_responses_train_strqa_sampled_responses_test_/16479_1_layerFalse_answer_last_transformer2_hallu_pos_bs288_epochs50_0.0005_Falseba,T101_/hl_llama_7B_/strqa_sampledplus_responses_train_strqa_sampled_responses_test_/16479_1_layerFalse_answer_last_transformer2_hallu_pos_bs288_epochs50_0.0005_Falseba,T2650_/hl_llama_7B_/strqa_sampledplus_responses_train_strqa_sampled_responses_test_/16479_1_layerFalse_answer_last_transformer2_hallu_pos_bs288_epochs50_0.0005_Falseba

Alp-Trivia:
dola+clap II (CLAP-wp) (get probes name): python analyse_crossdataset_bce_transfmr.py alpaca_7B trivia_qa --using_act layer --token answer_last --probes_file_name T42_/alpaca_7B_/ood_trivia_qa/_trivia_qa_dola16to32_responses_test_55000_1_layerFalse_answer_last_transformer_hallu_pos_bs352_epochs50_ --probes_file_name_concat ba --lr_list 0.00005,0.0005,0.005 --best_threshold True --save_path /home/local/data/ms/honest_llama_data --seed_list 42,101,2650 --fpr_at_recall -1
                                AUC: 
dola+clap II (CLAP-wp): python analyse_crossdataset_bce_transfmr.py alpaca_7B trivia_qa --using_act layer --token answer_last --probes_file_name T42_/alpaca_7B_/trivia_qa_greedy_responses_train5000_/5000_1_layerFalse_answer_last_transformer_hallu_pos_bs128_epochs50_ --probes_file_name_concat ba --lr_list 0.00005,0.0005,0.005 --best_threshold True --save_path /home/local/data/ms/honest_llama_data --seed_list 42,101,2650 --fpr_at_recall -1 --mitigated_responses_file_name dola16to32_responses_test --m_probes_file_name 
sampled+clap II (CLAP-wp) (get probes name): python analyse_crossdataset_bce_transfmr.py alpaca_7B trivia_qa --using_act layer --token answer_last --probes_file_name T42_/alpaca_7B_/trivia_qa_sampledplus_responses_train5000_trivia_qa_sampled_responses_validation1800_/55000_1_layerFalse_answer_last_transformer2_hallu_pos_bs352_epochs50_ --probes_file_name_concat ba --lr_list 0.00005,0.0005,0.005 --best_threshold True --save_path /home/local/data/ms/honest_llama_data --seed_list 42,101,2650 --fpr_at_recall -1 --test_num_samples 10
sampled+clap II (CLAP-wp): python analyse_crossdataset_bce_transfmr.py alpaca_7B trivia_qa --using_act layer --token answer_last --probes_file_name T42_/alpaca_7B_/trivia_qa_greedy_responses_train5000_/5000_1_layerFalse_answer_last_transformer_hallu_pos_bs128_epochs50_ --probes_file_name_concat ba --lr_list 0.00005,0.0005,0.005 --best_threshold True --save_path /home/local/data/ms/honest_llama_data --seed_list 42,101,2650 --fpr_at_recall -1 --mitigated_responses_file_name sampled_responses_labels_validation1800 --test_num_samples 10 --m_probes_file_name 

Alp-Str: 
dola+clap II (CLAP-wp) (get probes name): python analyse_crossdataset_bce_transfmr.py alpaca_7B strqa --using_act layer --token answer_last --probes_file_name T42_/alpaca_7B_/ood_strqa/_strqa_dola_0to16_responses_test_16479_1_layerFalse_answer_last_transformer_hallu_pos_bs288_epochs50_ --probes_file_name_concat ba --lr_list 0.00005,0.0005,0.005 --best_threshold True --save_path /home/local/data/ms/honest_llama_data --seed_list 42,101,2650 --fpr_at_recall -1
                                AUC: 
dola+clap II (CLAP-wp): python analyse_crossdataset_bce_transfmr.py alpaca_7B strqa --using_act layer --token answer_last --probes_file_name T42_/alpaca_7B_/strqa_baseline_responses_train_/1832_1_layerFalse_answer_last_transformer_hallu_pos_bs128_epochs50_ --probes_file_name_concat ba --lr_list 0.00005,0.0005,0.005 --best_threshold True --save_path /home/local/data/ms/honest_llama_data --seed_list 42,101,2650 --fpr_at_recall -1 --mitigated_responses_file_name dola_0to16_responses_test --m_probes_file_name 
sampled+clap II (CLAP-wp) (get probes name): python analyse_crossdataset_bce_transfmr.py alpaca_7B strqa --using_act layer --token answer_last --probes_file_name T42_/alpaca_7B_/strqa_sampledplus_responses_train_strqa_sampled_responses_test_/16479_1_layerFalse_answer_last_transformer2_hallu_pos_bs288_epochs50_ --probes_file_name_concat ba --lr_list 0.00005,0.0005,0.005 --best_threshold True --save_path /home/local/data/ms/honest_llama_data --seed_list 42,101,2650 --fpr_at_recall -1 --test_num_samples 8
sampled+clap II (CLAP-wp): python analyse_crossdataset_bce_transfmr.py alpaca_7B strqa --using_act layer --token answer_last --probes_file_name T42_/alpaca_7B_/strqa_baseline_responses_train_/1832_1_layerFalse_answer_last_transformer_hallu_pos_bs128_epochs50_ --probes_file_name_concat ba --lr_list 0.00005,0.0005,0.005 --best_threshold True --save_path /home/local/data/ms/honest_llama_data --seed_list 42,101,2650 --fpr_at_recall -1 --mitigated_responses_file_name sampled_responses_test --test_num_samples 8 --m_probes_file_name 