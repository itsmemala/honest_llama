# python get_activations_and_probe_non_linear_supcon_bce.py gemma_2B trivia_qa --train_file_name trivia_qa_greedy_responses_train5000 --test_file_name trivia_qa_greedy_responses_validation1800 --train_labels_file_name trivia_qa_greedy_responses_labels_train5000 --test_labels_file_name trivia_qa_greedy_responses_labels_validation1800 --len_dataset 5000 --num_folds 1 --using_act ah --token answer_last --method individual_linear_hallu_pos --bs 128 --epochs 50 --lr_list 0.0005,0.005 --save_probes True --save_path /home/local/data/ms/honest_llama_data --fast_mode True  --use_best_val_t True --best_using_auc True --seed_list 42,101,2650 --test_num_samples 1;
# python get_activations_and_probe_non_linear_supcon_bce.py gemma_2B nq_open --train_file_name nq_open_greedy_responses_train5000 --test_file_name nq_open_greedy_responses_validation1800 --train_labels_file_name nq_open_greedy_responses_labels_train5000 --test_labels_file_name nq_open_greedy_responses_labels_validation1800 --len_dataset 5000 --num_folds 1 --using_act ah --token answer_last --method individual_linear_hallu_pos --bs 128 --epochs 50 --lr_list 0.0005,0.005 --save_probes True --save_path /home/local/data/ms/honest_llama_data --fast_mode True  --use_best_val_t True --best_using_auc True --seed_list 42,101,2650 --test_num_samples 1;
# python get_activations_and_probe_non_linear_supcon_bce.py gemma_2B strqa --train_file_name strqa_baseline_responses_train --test_file_name strqa_baseline_responses_test --len_dataset 1832 --num_folds 1 --using_act ah --token answer_last --method individual_linear_hallu_pos --bs 128 --epochs 50 --lr_list 0.0005,0.005 --save_probes True --save_path /home/local/data/ms/honest_llama_data --fast_mode True  --use_best_val_t True --best_using_auc True --seed_list 42,101,2650 --test_num_samples 1;

python get_activations_and_probe_non_linear_supcon_bce.py llama3.1_8B_Instruct trivia_qa --train_file_name trivia_qa_greedy_responses_train5000 --test_file_name trivia_qa_greedy_responses_validation1800 --train_labels_file_name trivia_qa_greedy_responses_labels_train5000 --test_labels_file_name trivia_qa_greedy_responses_labels_validation1800 --len_dataset 5000 --num_folds 1 --using_act ah --token answer_last --method individual_linear_hallu_pos --bs 128 --epochs 50 --lr_list 0.0005,0.005 --save_probes True --save_path /home/local/data/ms/honest_llama_data --fast_mode True  --use_best_val_t True --best_using_auc True --seed_list 42,101,2650 --test_num_samples 1;
python get_activations_and_probe_non_linear_supcon_bce.py llama3.1_8B_Instruct nq_open --train_file_name nq_open_greedy_responses_train5000 --test_file_name nq_open_greedy_responses_validation1800 --train_labels_file_name nq_open_greedy_responses_labels_train5000 --test_labels_file_name nq_open_greedy_responses_labels_validation1800 --len_dataset 5000 --num_folds 1 --using_act ah --token answer_last --method individual_linear_hallu_pos --bs 128 --epochs 50 --lr_list 0.0005,0.005 --save_probes True --save_path /home/local/data/ms/honest_llama_data --fast_mode True  --use_best_val_t True --best_using_auc True --seed_list 42,101,2650 --test_num_samples 1;
python get_activations_and_probe_non_linear_supcon_bce.py llama3.1_8B_Instruct strqa --train_file_name strqa_baseline_responses_train --test_file_name strqa_baseline_responses_test --len_dataset 1832 --num_folds 1 --using_act ah --token answer_last --method individual_linear_hallu_pos --bs 128 --epochs 50 --lr_list 0.0005,0.005 --save_probes True --save_path /home/local/data/ms/honest_llama_data --fast_mode True  --use_best_val_t True --best_using_auc True --seed_list 42,101,2650 --test_num_samples 1;

# python get_activations_and_probe_non_linear_supcon_bce.py gemma_2B trivia_qa --train_file_name trivia_qa_greedy_responses_train5000 --test_file_name trivia_qa_sampled_responses_validation1800 --train_labels_file_name trivia_qa_greedy_responses_labels_train5000 --test_labels_file_name trivia_qa_sampled_responses_labels_validation1800 --len_dataset 5000 --num_folds 1 --using_act ah --token answer_last --method individual_linear_hallu_pos --bs 128 --epochs 50 --lr_list 0.0005,0.005 --save_probes True --save_path /home/local/data/ms/honest_llama_data --fast_mode True  --use_best_val_t True --best_using_auc True --seed_list 42,101,2650 --test_num_samples 10
# python get_activations_and_probe_non_linear_supcon_bce.py gemma_2B nq_open --train_file_name nq_open_greedy_responses_train5000 --test_file_name nq_open_sampled_responses_validation1800 --train_labels_file_name nq_open_greedy_responses_labels_train5000 --test_labels_file_name nq_open_sampled_responses_labels_validation1800 --len_dataset 5000 --num_folds 1 --using_act ah --token answer_last --method individual_linear_hallu_pos --bs 128 --epochs 50 --lr_list 0.0005,0.005 --save_probes True --save_path /home/local/data/ms/honest_llama_data --fast_mode True  --use_best_val_t True --best_using_auc True --seed_list 42,101,2650 --test_num_samples 10
# python get_activations_and_probe_non_linear_supcon_bce.py gemma_2B strqa --train_file_name strqa_baseline_responses_train --test_file_name strqa_sampled_responses_test --len_dataset 1832 --num_folds 1 --using_act ah --token answer_last --method individual_linear_hallu_pos --bs 128 --epochs 50 --lr_list 0.0005,0.005 --save_probes True --save_path /home/local/data/ms/honest_llama_data --fast_mode True  --use_best_val_t True --best_using_auc True --seed_list 42,101,2650 --test_num_samples 8

# python get_activations_and_probe_non_linear_supcon_bce.py llama3.1_8B_Instruct trivia_qa --train_file_name trivia_qa_greedy_responses_train5000 --test_file_name trivia_qa_sampled_responses_validation1800 --train_labels_file_name trivia_qa_greedy_responses_labels_train5000 --test_labels_file_name trivia_qa_sampled_responses_labels_validation1800 --len_dataset 5000 --num_folds 1 --using_act ah --token answer_last --method individual_linear_hallu_pos --bs 128 --epochs 50 --lr_list 0.0005,0.005 --save_probes True --save_path /home/local/data/ms/honest_llama_data --fast_mode True  --use_best_val_t True --best_using_auc True --seed_list 42,101,2650 --test_num_samples 10
# python get_activations_and_probe_non_linear_supcon_bce.py llama3.1_8B_Instruct nq_open --train_file_name nq_open_greedy_responses_train5000 --test_file_name nq_open_sampled_responses_validation1800 --train_labels_file_name nq_open_greedy_responses_labels_train5000 --test_labels_file_name nq_open_sampled_responses_labels_validation1800 --len_dataset 5000 --num_folds 1 --using_act ah --token answer_last --method individual_linear_hallu_pos --bs 128 --epochs 50 --lr_list 0.0005,0.005 --save_probes True --save_path /home/local/data/ms/honest_llama_data --fast_mode True  --use_best_val_t True --best_using_auc True --seed_list 42,101,2650 --test_num_samples 10
# python get_activations_and_probe_non_linear_supcon_bce.py llama3.1_8B_Instruct strqa --train_file_name strqa_baseline_responses_train --test_file_name strqa_sampled_responses_test --len_dataset 1832 --num_folds 1 --using_act ah --token answer_last --method individual_linear_hallu_pos --bs 128 --epochs 50 --lr_list 0.0005,0.005 --save_probes True --save_path /home/local/data/ms/honest_llama_data --fast_mode True  --use_best_val_t True --best_using_auc True --seed_list 42,101,2650 --test_num_samples 8