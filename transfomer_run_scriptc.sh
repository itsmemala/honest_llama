# python get_activations.py hl_llama_7B strqa --token answer_last --file_name strqa_baseline_responses_train  --device 0 --save_path /home/local/data/ms/honest_llama_data
# python get_activations.py hl_llama_7B strqa --token answer_last --file_name strqa_greedy_responses_validation1800  --device 0 --save_path /home/local/data/ms/honest_llama_data

python get_activations_and_probe_transfomer.py hl_llama_7B strqa --train_file_name strqa_baseline_responses_train --test_file_name strqa_baseline_responses_test --len_dataset 1832 --num_folds 1 --using_act layer --token answer_last --method transformer_hallu_pos --bs 128 --epochs 50 --lr 0.00005 --save_probes True --save_path /home/local/data/ms/honest_llama_data --fast_mode True  --plot_name transformer-n-0.00005b --tag main-transformer --use_best_val_t True 
python get_activations_and_probe_transfomer.py hl_llama_7B strqa --train_file_name strqa_baseline_responses_train --test_file_name strqa_baseline_responses_test --len_dataset 1832 --num_folds 1 --using_act layer --token answer_last --method transformer_hallu_pos --bs 128 --epochs 50 --lr 0.0005 --save_probes True --save_path /home/local/data/ms/honest_llama_data --fast_mode True  --plot_name transformer-n-0.0005b --tag main-transformer --use_best_val_t True 

python get_activations_and_probe_transfomer.py hl_llama_7B strqa --train_file_name strqa_baseline_responses_train --test_file_name strqa_baseline_responses_test --len_dataset 1832 --num_folds 1 --using_act layer --token answer_last --method transformer_hallu_pos --bs 128 --epochs 50 --lr 0.00005 --save_probes True --save_path /home/local/data/ms/honest_llama_data --fast_mode True  --plot_name 101-transformer-n-0.00005b --tag main-transformer --use_best_val_t True --seed 101
python get_activations_and_probe_transfomer.py hl_llama_7B strqa --train_file_name strqa_baseline_responses_train --test_file_name strqa_baseline_responses_test --len_dataset 1832 --num_folds 1 --using_act layer --token answer_last --method transformer_hallu_pos --bs 128 --epochs 50 --lr 0.0005 --save_probes True --save_path /home/local/data/ms/honest_llama_data --fast_mode True  --plot_name 101-transformer-n-0.0005b --tag main-transformer --use_best_val_t True --seed 101

python get_activations_and_probe_transfomer.py hl_llama_7B strqa --train_file_name strqa_baseline_responses_train --test_file_name strqa_baseline_responses_test --len_dataset 1832 --num_folds 1 --using_act layer --token answer_last --method transformer_hallu_pos --bs 128 --epochs 50 --lr 0.00005 --save_probes True --save_path /home/local/data/ms/honest_llama_data --fast_mode True  --plot_name 2650-transformer-n-0.00005b --tag main-transformer --use_best_val_t True --seed 2650
python get_activations_and_probe_transfomer.py hl_llama_7B strqa --train_file_name strqa_baseline_responses_train --test_file_name strqa_baseline_responses_test --len_dataset 1832 --num_folds 1 --using_act layer --token answer_last --method transformer_hallu_pos --bs 128 --epochs 50 --lr 0.0005 --save_probes True --save_path /home/local/data/ms/honest_llama_data --fast_mode True  --plot_name 2650-transformer-n-0.0005b --tag main-transformer --use_best_val_t True --seed 2650

python get_activations.py hl_llama_7B strqa --token answer_last --file_name strqa_sampledplus_responses_train  --device 0 --save_path /home/local/data/ms/honest_llama_data
# python get_activations.py hl_llama_7B strqa --token answer_last --file_name strqa_greedy_responses_validation1800  --device 0 --save_path /home/local/data/ms/honest_llama_data

python get_activations_and_probe_transfomer.py hl_llama_7B strqa --train_file_name strqa_sampledplus_responses_train --test_file_name strqa_baseline_responses_test --len_dataset 16479 --num_folds 1 --using_act layer --token answer_last --method transformer_hallu_pos --bs 128 --epochs 50 --lr 0.00005 --save_probes True --save_path /home/local/data/ms/honest_llama_data --fast_mode True  --plot_name transformer-n*aug-0.00005b --tag main-transformer --use_best_val_t True 
python get_activations_and_probe_transfomer.py hl_llama_7B strqa --train_file_name strqa_sampledplus_responses_train --test_file_name strqa_baseline_responses_test --len_dataset 16479 --num_folds 1 --using_act layer --token answer_last --method transformer_hallu_pos --bs 128 --epochs 50 --lr 0.0005 --save_probes True --save_path /home/local/data/ms/honest_llama_data --fast_mode True  --plot_name transformer-n*aug-0.0005b --tag main-transformer --use_best_val_t True 

python get_activations_and_probe_transfomer.py hl_llama_7B strqa --train_file_name strqa_sampledplus_responses_train --test_file_name strqa_baseline_responses_test --len_dataset 16479 --num_folds 1 --using_act layer --token answer_last --method transformer_hallu_pos --bs 128 --epochs 50 --lr 0.00005 --save_probes True --save_path /home/local/data/ms/honest_llama_data --fast_mode True  --plot_name 101-transformer-n*aug-0.00005b --tag main-transformer --use_best_val_t True --seed 101
python get_activations_and_probe_transfomer.py hl_llama_7B strqa --train_file_name strqa_sampledplus_responses_train --test_file_name strqa_baseline_responses_test --len_dataset 16479 --num_folds 1 --using_act layer --token answer_last --method transformer_hallu_pos --bs 128 --epochs 50 --lr 0.0005 --save_probes True --save_path /home/local/data/ms/honest_llama_data --fast_mode True  --plot_name 101-transformer-n*aug-0.0005b --tag main-transformer --use_best_val_t True --seed 101

python get_activations_and_probe_transfomer.py hl_llama_7B strqa --train_file_name strqa_sampledplus_responses_train --test_file_name strqa_baseline_responses_test --len_dataset 16479 --num_folds 1 --using_act layer --token answer_last --method transformer_hallu_pos --bs 128 --epochs 50 --lr 0.00005 --save_probes True --save_path /home/local/data/ms/honest_llama_data --fast_mode True  --plot_name 2650-transformer-n*aug-0.00005b --tag main-transformer --use_best_val_t True --seed 2650
python get_activations_and_probe_transfomer.py hl_llama_7B strqa --train_file_name strqa_sampledplus_responses_train --test_file_name strqa_baseline_responses_test --len_dataset 16479 --num_folds 1 --using_act layer --token answer_last --method transformer_hallu_pos --bs 128 --epochs 50 --lr 0.0005 --save_probes True --save_path /home/local/data/ms/honest_llama_data --fast_mode True  --plot_name 2650-transformer-n*aug-0.0005b --tag main-transformer --use_best_val_t True --seed 2650

